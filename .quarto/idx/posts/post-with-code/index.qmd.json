{"title":"Does GPT2 correlate with your brain?","markdown":{"yaml":{"title":"Does GPT2 correlate with your brain?","author":"dermot","date":"2022-11-16","categories":["code","analysis"],"image":"image2.webp","bibliography":"references.bib"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n![Possibly the author on the left.](image2.webp \"Title\")\n\n\n\nLarge Language Models (LLMs) have had quite a run over the past few years. Not only have they crushed most existing language benchmarks into the dust, but recent iterations have excelled at everything from groking high school math questions and writing papers for overworked undergraduates, to even convincing google engineers to retain lawyers on their behalf. With further scaling, it sometimes seems that full AGI will be arriving as soon as somebody stumps up the cash to wire just enough GPUs in parallel, and feed them the entirety of the internet as training data. \n\nPerhaps surprisingly, much of the above has emerged in the absence of any particular training goal. The networks aren't optimized to write coherent undergrad papers, or to exhibit convicing portrayals of embodied agency. They are tasked only with predicting the next most likely word in a sequence, given some previous context (e.g an incomplete sentence). All the rest seems to appear 'for free' (or with a lot of clever prompting). \n\nIf this isn't enough, a number of recent papers have claimed that existing LLMs actually predict human brain activity during language comprehension [@caucheteux2022deep; @schrimpf2021neural]. Despite never seeing a single blip of human neurology, it seems that their internal activations reliably correlate with the parts of the brain considered to be active during language processing. This might seem to be taking things a little too far - LLMs are so amazing that they can even predict your brain! With a healthy dose of skepticism, I'll examine this claim by unpacking the source code of perhaps the best cited of these papers, Schrimpf et al 2021.\n\n### Schrimpf et al's pipeline\n\nThe Schrimpf et al paper tests a variety of pretrained LLM models on neural datasets, finding openAI's gpt2 (which is the largest)[^5] to be the best. The authors follow a basic pipeline:\n\n1. A dataset is chosen containing observed FMRI values for participants as they listen to recorded text. The dataset is arranged so that each group of FMRI values is matched with the sentence that was being played when the corresponding FMRI values were recorded[^3].\n\n2. The sentences are grouped into stories. The stories are passed into gpt2 (or a smaller LLM), one sentence at a time, but including the previous sentences in the story as contextual input, and the LLM's activations are recorded at each layer.\n\n3. Thus we have two datasets. One is a set of FMRI values paired with sentences, and the other is a set of gpt2 activations paired with the same sentences.\n\n4. For each column of FMRI values, and for each layer of gpt2, they fit a Linear Regression model between gpt2's activations at that layer, and the column of FMRI values. A train/test split is used.\n\n5. Each of these models is evaluated on the pearson corrleation between the real and predicted FMRI values for the holdout test data.\n\n6. For each layer, the correlations are aggregated across columns of FMRI values, and then normalized via a noise ceiling to produce a 'brain score' for that layer.\n\n### Running Schrimpf et al's code on [Blank 2014]( https://journals.physiology.org/doi/full/10.1152/jn.00884.2013)\n\nSchrimpf et al are kind enough to make their code available [here](https://github.com/mschrimpf/neural-nlp). I had some trouble getting it installed on my machine[^1], but once it's up and running the pipeline can be executed more or less like this:\n\n```python\nfrom neural_nlp import score as score_function\nfrom neural_nlp.models import model_pool\n\nmodel='gpt2-xl'\nbenchmark = 'Blank2014fROI-encoding'\nlayers=None\nsubsample=None\n\n\n(raw, normalized )= score_function(model=model,\n            layers=layers,\n            subsample=subsample,\n            benchmark=benchmark)\n\n\nprint(list([np.round(x,4) for x in raw.mean(axis=1).values]))\n\n--->[0.0063, 0.0063, 0.0237, 0.0249, 0.0378, 0.0342, 0.0228, 0.0241, 0.022, 0.0185, 0.0259,\n 0.022, 0.0245, 0.0283, 0.03, 0.0409, 0.0424, 0.0373, 0.0422, 0.0425, 0.0447, 0.0374, 0.0437,\n  0.0442, 0.0518, 0.0503, 0.0559, 0.0594, 0.0582, 0.0619, 0.0519, 0.0539, 0.0569, 0.0548, 0.0498,\n   0.0468, 0.0504, 0.049, 0.0457, 0.0515, 0.0483, 0.0475, 0.0472, 0.0473, 0.0438, 0.041, 0.0407, \n   0.0385, 0.0366]\n\nprint([np.round(x,4) for x in normalized.values[:,0]])\n\n--->[0.0513, 0.0203, 0.1216, 0.1678, 0.2002, 0.1792, 0.1091, 0.1085, 0.1189, 0.0806, 0.1171,\n 0.0688, 0.0999, 0.1618, 0.179, 0.1798, 0.2094, 0.1826, 0.1667, 0.2088, 0.2309, 0.1709,\n  0.206, 0.2154, 0.265, 0.2758, 0.3062, 0.2916, 0.367, 0.4073, 0.3394, 0.3412, 0.3865, 0.3738,\n   0.3075, 0.2637, 0.3211, 0.2725, 0.2605, 0.2515, 0.2349, 0.2225, 0.2003, 0.2031,\n    0.2366, 0.2183, 0.2253, 0.1911, 0.1847]\n\n---->\n```\n\n![The output](run1.png \"Title\")\n\nAfter many hours of compute, we get the above results. The output is two instances of `DataAssembly`, the first one giving the raw correlations, and the second providing the final brain scores. We can see that they're at their strongest (max `0.4073`) in the middle to latter layers of gpt2. So this seems ok. The raw correlations are really small (get back to this later), though Schrimpf et al assure us that this is quite good compared to the noise ceiling. But I'm not so easily convinced. Remember, that gpt2 has still never seen a single real life brain!\n\n### Signal Autocorrelation and a random embedding trick.\n\nWhat if these FMRI signals were to correlate with themselves over time? For instance, your current heart beat is a reasonably good indicator of what your heart beat will be ten seconds from now. Its going to do better than if I just make a blind guess, given no prior knowledge. Similarly, the FMRI signal value at one time point might not be a particularly bad predictor of the next timepoint. We can see how this works out for the FMRI data below.\n\n```python\nimport numpy as np\nfrom scipy.stats import pearsonr\nbenchmark=benchmark_pool['Blank2014fROI-encoding']\ndata = np.array(benchmark._target_assembly)\ncorrs=[]\nskip=1\n\n#for each FMRI column, lag the signal by one step and calculate correlation\n#with the unlagged signal\n\nfor i in range(data.shape[1]):\n    #lag the signal by 'skip' steps\n    v=data[:,i]\n    x=v[:-skip]\n    y=v[skip:]\n    cor = pearsonr(x,y)[0]\n    corrs.append(cor)\nprint(np.mean(corrs))\n\n---> 0.41318839674224245\n```\n\n They do. Given that, we might wonder if there is a way we could trick a linear regression on random data to predict the right signal. Well seemingly we can. Below, I initialize a random embedding at the beginning of each story and then proceed by injecting a small amount of noise at each time step. For each voxel dimension, I then randomly split the data into 0.9/0.1 train/test and train a linear regression to predict the voxels from the random embeddings. \n\n```python\nfrom sklearn.linear_model import LinearRegression\n\n#get the story name for each sample\nstimuli_ids = data._target_assembly['stimulus_id'].values\nstimuli_ids = [x.split('.')[0] for x in stimuli_ids]\n\nembed_dim=3000\nembeds=[]\n#make a random embedding\ncur_embed = np.random.normal(0, 1, [embed_dim])\nprev_name = 'A'\nfor stimuli_id in stimuli_ids:\n    #if stimuli_id is a new story, create a new random embedding\n    if prev_name != stimuli_id:\n        cur_embed = np.random.normal(0, 1, [embed_dim])\n    #otherwise, just add a (fairly large) amount of noise to the current embedding  \n    else:\n        cur_embed = cur_embed + np.random.normal(0, 0.1, [embed_dim])\n    prev_name=stimuli_id\n    embeds.append(cur_embed)\n    \nX=np.array(embeds)\ncorrs=[]\n\n#for each voxel, do a random train test split\n#build a linear regression between the semi random embeddings and the target\nfor i in range(data.shape[1]):\n    Y = data[:, i]\n    train_idx = np.random.random(len(X))<0.9\n    train_x = X[train_idx]\n    train_y = Y[train_idx].reshape(-1,1)\n    test_x = X[~train_idx]\n    test_y = Y [~train_idx].reshape(-1,1)\n    m = LinearRegression().fit(train_x, train_y)\n    pred = m.predict(test_x)\n    cor,p = pearsonr(pred.flatten(), test_y.flatten())\n    corrs.append(cor)\n\nprint('Mean correlation with simple train/test split', np.mean(corrs))\n\n---> Mean correlation with simple train/test split 0.41665761324952694\n```\n\nSo my simple embedding scheme effectively captures all of the autocorrelation implicit in the signals, and does much better than gpt2 in terms of raw correlation. We can bake this into Shcrimpf et al's code with a quick hack:\n\n```python\nimport neural_nlp\nfrom neural_nlp import score as score_function\nfrom neural_nlp.models import model_pool\nimport numpy as np\nimport itertools\nfrom neural_nlp.utils import ordered_set\nfrom brainio.assemblies import DataAssembly, walk_coords, merge_data_arrays, array_is_element\nfrom neural_nlp.benchmarks.neural import Blank2014fROIEncoding\n\ndef listen_to(candidate, stimulus_set, reset_column='story', average_sentence=True):\n    \"\"\"\n    Code is from neural_nlp.benchmarks.neural.listen_to\n    Add code to replace story_activations with our random trick embeddings of the same size.\n    \n    Pass a stimulus_set through a model candidate.\n    Operates on a sentence-based stimulus_set.\n    \"\"\"\n    activations = []\n    for story in ordered_set(stimulus_set[reset_column].values):\n        story_stimuli = stimulus_set[stimulus_set[reset_column] == story]\n\n        story_stimuli.name = f\"{stimulus_set.name}-{story}\"\n        story_activations = candidate(stimuli=story_stimuli, average_sentence=average_sentence)\n        \n        #This is the only addition to the code - add our random trick in place of\n        #gpt2's activations\n        embed=np.random.normal(0,1 , [story_activations.shape[1]])\n        new_activ = [embed]\n        for i in range(1,story_activations.shape[0]):\n            embed = embed + np.random.normal(0, 0.001, story_activations.shape[1])\n            new_activ.append(embed)\n        story_activations[:] = new_activ\n        #End addition.\n        \n        \n        activations.append(story_activations)\n        \n    model_activations = merge_data_arrays(activations)\n    # merging does not maintain stimulus order. the following orders again\n    idx = [model_activations['stimulus_id'].values.tolist().index(stimulus_id) for stimulus_id in\n           itertools.chain.from_iterable(s['stimulus_id'].values for s in activations)]\n    assert len(set(idx)) == len(idx), \"Found duplicate indices to order activations\"\n    model_activations = model_activations[{'presentation': idx}]\n    #the model activations returned here are actually ordered by story\n   \n    return model_activations\n\n\n#need to change __call__ method to use the listen_to method above\nclass Blank2014RandomEmbed(Blank2014fROIEncoding):\n    \n    def __call__(self, candidate):\n\n        model_activations = listen_to(candidate, self._target_assembly.attrs['stimulus_set'])\n        assert set(model_activations['stimulus_id'].values) == set(self._target_assembly['stimulus_id'].values)\n        score = self.apply_metric(model_activations, self._target_assembly).copy()\n        \n        normalized_score,raw_score = self.ceiling_normalize(score)\n    \n\n        return raw_score, normalized_score\n    \n\nbenchmark_pool['Blank2014RandomEmbed'] = BlankRandomEmbed('Blank2014fROI-encoding')\n\nmodel='gpt2-xl'\nlayers=None\nsubsample=None\nbenchmark='Blank2014RandomEmbed'\n\n(raw, normalized )= score_function(model='gpt2-xl',\n            layers=layers,\n            subsample=subsample,\n            benchmark=benchmark)\n            \n            \nprint([np.round(x,4) for x in normalized.values[:,0]])\n---->[1.6771, 1.7417, 1.5778, 1.7718, 1.6413, 1.6526, 1.6702, 1.6343,\n 1.6861, 1.6316, 1.6659, 1.621, 1.7315, 1.7783, 1.7054, 1.6932, 1.5978,\n  1.7578, 1.6671, 1.7184, 1.6518, 1.6154, 1.6411, 1.7644, 1.5474, 1.5287, \n  ]1.6716, 1.7877, 1.7792, 1.707, 1.7296, 1.6836, 1.7145, 1.7302, 1.7645, \n  1.7717, 1.568, 1.5788, 1.7494, 1.7074, 1.7392, 1.7495, 1.6844,\n1.7248, 1.6814, 1.6247, 1.6582, 1.6814, 1.7328]\n```\n![Brain-scores for the random embeddings](embeddingtrick.png 'Title')\n\nThe brain scores here are huge! They completely destroy those of gpt2! So what is going on? If my random embedding trick can do this well within the authors framework, then how can it be said to be adequately testing for anything at all?\n\n### So what allowed for the 'good' results?\n\nMy random embedding scheme relied upon the fact that each embedding in a story was 'similar' to its parent and child. I did this by injecting only a small amount of noise at each time step. If we think about it, the same thing should apply to gpt2's activations. The learned representation of the sentence \"The quick brown fox jumps over the lazy dog\" should be 'similar' to \"The quick brown fox jumps over the lazy dog and then goes in search of...\". After all, not an awful lot has really changed. \n\nI tried to check this by calculating the 1-step cosine similarity values for the activations gpt2 produced - e.g the similarity between the embedding for \"The quick brown fox jumps over the lazy dog\" and that for \"The quick brown fox jumps over the lazy dog and then goes in search of...\" . This didn't work as expected. The lower (worst performing) layers of gpt2 had the best interstep similarity, which is contrary to what I would have expected from the results. So who knows - maybe cosine similarity just isn't a useful measure for LLM embeddings. \n\nWhat I was able to do though, was show that gpt2's layer activations would exhibit similar behaviour when tested against random autocorrelated signals that I generated off the bat:\n\n```python\nfrom neural_nlp.benchmarks import benchmark_pool\nfrom scipy.stats import pearsonr\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport pickle\n\n#Thanks to https://stackoverflow.com/a/33904277 for the sample_signal code\ndef sample_signal(n_samples, corr, mu=0, sigma=1):\n    assert 0 < corr < 1, \"Auto-correlation must be between 0 and 1\"\n\n    # Find out the offset `c` and the std of the white noise `sigma_e`\n    # that produce a signal with the desired mean and variance.\n    # See https://en.wikipedia.org/wiki/Autoregressive_model\n    # under section \"Example: An AR(1) process\".\n    c = mu * (1 - corr)\n    sigma_e = np.sqrt((sigma ** 2) * (1 - corr ** 2))\n\n    # Sample the auto-regressive process.\n    signal = [c + np.random.normal(0, sigma_e)]\n    for _ in range(1, n_samples):\n        signal.append(c + corr * signal[-1] + np.random.normal(0, sigma_e))\n\n    return np.array(signal)\n\ndef compute_corr_lag_1(signal):\n    return np.corrcoef(signal[:-1], signal[1:])[0][1]\n\n\ndef load(f):\n    \n    with open(f, 'rb') as handle:\n      \n        return pickle.load(handle)\n\n\n\nlayer_scores=[]\n#create a random autocorrelated signal in the place of each neuroid\nsignal=[ sample_signal(len(data),0.5) for j in range(50)]\n\nfor i in range(1, 50):\n    \n    #I saved the layer activations to disk.\n    activations = load('activations/{}.bin'.format(str(i))).values\n    \n    corrs=[]\n    for j in range(50):\n\n        #80/20 train/test plit\n        train_idx = np.random.random(data.shape[0])<0.8 \n        test_idx = ~train_idx\n        train_X = activations[train_idx]\n        train_Y = signal[j].reshape(-1,1)[train_idx]\n        test_X = activations[test_idx]\n        test_Y = signal[j].reshape(-1,1)[test_idx]\n\n        #fit model between gpt2 and the random signal\n        model = LinearRegression().fit(train_X, train_Y)\n        #predict and evaluate\n        pred = model.predict(test_X)\n        cor, p = pearsonr(pred.flatten(), test_Y.flatten())\n        corrs.append(cor)\n        \n  \n    layer_scores.append(np.mean(corrs))\n        \nprint([np.round(f,4) for f in list(layer_scores)]) \n\n---> [0.0032, 0.0014, 0.0198, 0.0264, 0.034, 0.0213, 0.0246, 0.0213, 0.0306,\n 0.03, 0.018, 0.0069, 0.0136, 0.0294, 0.0184, 0.0232, 0.025, 0.0365, 0.0338, \n 0.0464, 0.0588, 0.0526, 0.0419, 0.0442, 0.0619, 0.0636, 0.0553, 0.0373,\n  0.0368, 0.0485, 0.0614, 0.0494, 0.0541, 0.0519, 0.0507, 0.0511, 0.0563, \n  0.0651, 0.0482, 0.049, 0.0468, 0.0589, 0.0565, 0.0656, 0.0531, 0.0634, \n  0.0515, 0.0704, 0.0357]\n```\n\n![Gpt2 correlation with random signal](randomsignal.png 'Title')\n\nThe raw correlation scores get better as you go deeper into the model, and then decrease towards the end. This doesn't happen quite as smoothly as when fitting gpt2 to the FMRI data. But there you go. Not only can a set of random embeddings excel within the paper's pipeline, but gpt2 can 'predict' more or less any autocorrelated signal, when tested in this way.\n\n### Examining the train/test split.\n\nYou might have noticed that I've been performing random train/test splits, with no regards as to the 'series' nature of the  data. And of course, the authors do this [too](https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184). The option that the authors default to is:\n\n`self._split = StratifiedKFold(n_splits=splits, shuffle=True, random_state=random_state)`[^2]\n\nBut this is definitely not how you're meant to handle time series data. The use of `shuffle=True` means that the voxel values from the end of each story are mixed up with those at the beginning, and the test values are just randomly missing points in the time series.\n\n\nIf we wanted to just use a consecutive K-Fold cross validation, we could fix it like this:\n\n```python\n...\nself._split = KFold(n_splits=splits, shuffle=False)\n...\n```\nRecomputing is quick, as the previously calculated gpt2 activations have been stored.\n```python\n...\n...\nprint(list([np.round(x,4) for x in raw.mean(axis=1).values]))\n--->[0.0099, 0.0065, 0.003, -0.0029, 0.0054, 0.0101, 0.0051, 0.0047, -0.0082, 0.0025,\n 0.0047, 0.0088, 0.008, 0.0067, 0.0053, 0.0134, 0.0086, 0.0041, 0.0133,\n  0.0174, 0.0131, 0.0053, 0.0045, 0.0043, 0.0075, 0.0127, 0.0147, 0.0166,\n   0.0142, 0.0169, 0.0151, 0.0124, 0.0179, 0.0143, 0.0076, 0.0002, -0.0007, \n   -0.0005, -0.0002, 0.0005, -0.0019, -0.0001, -0.0018, 0.0008, 0.0022, -0.0002,\n    0.0029, 0.0018, 0.0042]\n\nprint([np.round(x,4) for x in normalized.values[:,0]])\n--->[0.0486, 0.0586, 0.0251, -0.017, 0.075, 0.0555, 0.0691,\n 0.0358, -0.041, 0.0167, 0.0174, 0.0582, 0.0196, 0.0136,\n  0.0446, 0.0415, 0.0772, 0.063, 0.0271, 0.1104, 0.0601,\n   0.013, 0.0308, -0.031, 0.0151, 0.0978, 0.0963, 0.1133,\n    0.1328, 0.1853, 0.1706, 0.0971, 0.1552, 0.1403, 0.023, \n    0.0135, 0.0109, -0.0029, -0.0107, -0.0227, -0.0169,\n     0.0412, 0.0203, 0.0424, -0.0042, -0.0253, 0.0212, 0.0083, 0.0184]\n```\n\nThose scores are really different. But I think there is still some possibility of overlap occurring - future values being used as training data for test values earlier in the series. Another option is to do away with cross validation and just use a single split, using the first 80% of a each story as training data, and the last 20% as test. Thus we ensure that no future data is seen by the linear regressions. This is assuming that the stories constitute breaks in the time series. It's hacky, but we pop this change into the brain_score code [here](https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L311) so that we run the cross validation 5 times on the same split.\n\n```python\n...\n...\n...\ntrain_idx = []\ntest_idx=[]\nfrom collections import defaultdict\nd=defaultdict(list)\nstimulus_ids = target_assembly['stimulus_id'].values\nfor idx, stimulus_id in enumerate(stimulus_ids):\n\n    stimulus_id=stimulus_id.split('.')[0]\n    d[stimulus_id].append(idx)\n\nfor stimulus_id in d:\n    idxs=d[stimulus_id]\n    cutoff=int(len(idxs)*0.8)\n    train_idx.extend(list(idxs[:cutoff]))\n    test_idx.extend(list(idxs[cutoff:]))\n\nfor split_iterator, (train_indices, test_indices), done \\\n                in tqdm(enumerate_done(splits), total=len(splits), desc='cross-validation'):\n            \n    train_indices,test_indices = train_idx, test_idx\n\n...\n...\n...\n\nprint(list([np.round(x,4) for x in raw.mean(axis=1).values]))\n--->[0.0022, 0.0179, 0.008, 0.0018, 0.0106, 0.019, 0.0127, 0.0126, 0.0002,\n 0.0037, 0.0047, 0.0111, 0.0089, 0.0147, 0.0069, 0.0161, 0.0131, 0.0064, \n 0.0164, 0.0191, 0.015, 0.0075, 0.0083, 0.0086, 0.0142, 0.0172, 0.0207, \n 0.0222, 0.0207, 0.0226, 0.0204, 0.0202, 0.0269, 0.023, 0.0183, 0.0088, \n 0.0094, 0.0092, 0.0062, 0.0065, 0.0043, 0.0041, -0.0002, 0.0023, 0.0063, \n 0.0045, 0.005, 0.0034, 0.0083]\n\nprint([np.round(x,4) for x in normalized.values[:,0]])\n--->[0.0268, -0.0866, 0.1223, -0.0465, -0.1973, 0.0199, \n-0.0719, 0.0215, -0.0109, -0.1299, 0.083, -0.0626, -0.0418, \n-0.0597, 0.0077, 0.0426, -0.0827, 0.0612, 0.0772, 0.1293, \n0.0654, -0.0816, 0.0846, -0.017, 0.1064, 0.0623, 0.0701, \n0.046, 0.1553, 0.1693, 0.0432, 0.1265, 0.008, 0.0444, 0.0082, \n-0.0185, -0.0137, -0.0178, 0.0317, 0.0407, -0.0013, 0.0121, \n-0.0307, 0.0045, 0.0989, -0.1037, -0.0928, -0.153, -0.0165]\n```\n\nThat's a big difference. The brain scores have pretty much collapsed, and there are a load of negative scores now. Alternatively, we could consider the whole dataset as one consecutive time series, and train on the first 80% of values and test only on the final 20%. E.g:\n\n```python\n...\n...\nidx = np.arange(target_assembly.shape[0])\ncut_off = int(len(idx)*0.8)\ntrain_idx = idx[:cut_off]\ntest_idx = idx[cut_off:]\n...\n...\n\nprint(list([np.round(x,4) for x in raw.mean(axis=1).values]))\n--->[0.0014, 0.0081, 0.0092, -0.001, 0.0131, 0.0026, -0.0079, 0.0155, -0.0059, \n0.0062, 0.0066, 0.0328, 0.0234, 0.0069, 0.0055, 0.0235, 0.0118, 0.0146, 0.0236, \n0.0262, 0.0165, 0.0062, 0.0096, 0.0086, 0.0072, 0.0194, 0.0251, 0.0239, 0.0125, \n0.0108, 0.0156, 0.0142, 0.0334, 0.0216, 0.0056, -0.0068, -0.0012, 0.001, 0.0057, \n0.0014, 0.0007, -0.0181, -0.0114, -0.0016, -0.0023, -0.0104, -0.009, -0.0104, -0.002]\n\nprint([np.round(x,4) for x in normalized.values[:,0]])\n--->[-0.0213, 0.0667, -0.0945, 0.0035, 0.0844, -0.0822, -0.0548, 0.18, \n0.0561, 0.0135, 0.0509, 0.2229, 0.0795, 0.0935, 0.0614, 0.1577, 0.0676,\n 0.1072, 0.1658, 0.1828, 0.0529, 0.0734, 0.0523, 0.0425, 0.0354, 0.126, \n 0.1824, 0.0505, 0.0899, 0.0414, 0.0949, 0.124, 0.2261, 0.2791, 0.1468, 0.0557, \n 0.0382, 0.0193, 0.0594, 0.0374, 0.0412, -0.02, -0.0065, -0.0035, 0.0318, -0.0479, -0.0127, 0.0144, 0.0567]\n\n```\n\n![Brain score comparisons across split strategies](finalcomparison.png 'Title')\n\nAs the gpt2 activations have been saved, it all recomputes pretty quickly, and we get the above results. The mean normalized brain_score across layers in the first instance was `0.21432`. For the KFold fix, it went down to `0.044738`. Splitting the stories gave us `0.008828`, and just naively splitting the whole dataset 80/20 gives us `0.06575102040816326`. And look at the raw correlation values! There are still one or two layers showing some layer/brain correlation, but the values are tiny. But, a correlation is still a correlation. Or is it?\n\n### What makes a significant brain-score?\n\nI wanted to look a bit further, so I unpacked the actual p-values for the individual correlations that the best scoring gpt2 layer produced.\n\n```python\nactivations=load('activations/{}.bin'.format(str(best_layer+1))).values\nidx = np.arange(data.shape[0])\ncut_off = int(len(idx)*0.8)\ntrain_idx=idx[:cut_off]\ntest_idx=idx[cut_off:]\ntrain_X = activations[train_idx]\ntrain_Y = data.values[train_idx]\ntest_X = activations[test_idx]\ntest_Y = data.values[test_idx]\nmodel = LinearRegression().fit(train_X, train_Y)\npred = model.predict(test_X)\nps=[]\n\nfor j in range(data.shape[1]):\n\n    cor, p = pearsonr(pred[:,j].flatten(), test_Y[:,j].flatten())\n    ps.append(p)\n\nprint(ps)\nprint(np.mean(ps))\n\n---->[0.1261, 0.4546, 0.7403, 0.4028, 0.1986, 0.8898, 0.2407,\n 0.5314, 0.4364, 0.9006, 0.8181, 0.4686, 0.0237, 0.3228, 0.7242,\n  0.3558, 0.0549, 0.1904, 0.9157, 0.3441, 0.1256, 0.0802, 0.5643,\n   0.4628, 0.1521, 0.9165, 0.2372, 0.6676, 0.9878, 0.9566, 0.6633,\n    0.1939, 0.2056, 0.6876, 0.7831, 0.2193, 0.9157, 0.0956, 0.8299,\n     0.867, 0.4778, 0.8489, 0.6799, 0.3197, 0.7199, 0.7147, 0.6371, \n     0.8556, 0.374, 0.103, 0.8697, 0.2803, 0.4908, 0.0162, 0.7662,\n      0.3224, 0.6696, 0.0839, 0.0171, 0.9656]\n```\n\nSo they're really high. Really, really high. If the significance threshold is a conservative 0.05, only 3 voxels actually test out below that. Most of them look like garbage.\n\nBut what do we do with these p-values? How can we judge the significance of the overall brain-score (average correlation)?\n\nA straight up approach might suggest that we're testing multiple different hypotheses (e.g gpt2 correlates with voxel 1, with voxel 2... etc). If we follow the [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction) method, then we would end up setting our significance threshold 60 times lower, and reject everything. But that can't be how people generally treat brain voxels right? \n\nAs per wikipedia, multiple comparisons assumes that we are testing quite different hypotheses e.g a new teaching method improves spelling, arithmetic, reading comprehension. But these voxels, they're kind of like similar hypotheses right? So perhaps the p-value for the final brain score should end up lower overall?\n\nI couldn't decide. I thought maybe we could empirically estimate p-values for the brain-scores by running the test thousands of times but with randomly generated gpt2 activations. Then we can see an approximate likelihood of doing as well as gpt2 with completely uncorrelated data. It's a bit rough and ready, but I guess it will do in the absence of something better.\n\n```python\nbrain_scores = []\n# Compute the average correlation between random activations and the brain data\nfor i in range(1000):\n    \n    idx = np.arange(data.shape[0])\n    cut_off = int(len(idx)*0.8)\n    train_idx=idx[:cut_off]\n    test_idx=idx[cut_off:]\n    activations = np.random.normal(0, 0.1, activations.shape)\n    train_X = activations[train_idx]\n    train_Y = data.values[train_idx]\n\n    test_X = activations[test_idx]\n    test_Y = data.values[test_idx]\n    model = LinearRegression().fit(train_X, train_Y)\n    pred = model.predict(test_X)\n    corrs=[]\n    ps=[]\n  \n    for j in range(data.shape[1]):\n\n        cor, p = pearsonr(pred[:,j].flatten(), test_Y[:,j].flatten())\n        corrs.append(cor)\n    \n        \n   \n    brain_scores.append(np.mean(corrs))\n\n#sort the average correlations and plot as p-value/average-correlation\nsorted_brain_scores=sorted(brain_scores)\nsorted_brain_scores=np.flip(sorted_brain_scores)[:200]\nx=np.arange(len(sorted_brain_scores))*0.001\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.plot(x, sorted_brain_scores)\nplt.xlabel('p-value')\nplt.ylabel('correlation')\nplt.show()\n```\n\n![Average-Correlation/p-value plot](pvalues2.png 'Title')\n\nSo if that's right, an average_correlation at or above `0.0195` is 'significant', at `p<0.05`. That puts several layers of gpt2 well within significance territory. \n\nStill, doesn't testing 50 different gpt2 activation layers sound an awful lot like testing 50 different hypothesis? If so, then applying Bonferroni correction we'd be adjusting the significance threshold down to `p<=0.001`, or an average correlation of `0.0412`. That way we'd once again end up rejecting everything.\n\nI can't decide. I guess gpt2 might correlate with your brain, but only very, very slightly.\n\n### Conclusion\n\nAltogether, this paper hasn't convinced me that gpt2 knows anything about my brain. There are some caveats though. As mentioned[^4], I couldn't get the package to install cleanly, so I would be a little wary of version mismatches. The choice of train/test split was a pretty obvious mistake, but I am unsure as to whether what's left afterwards is significant or not. Also, I have only tested one dataset, whereas the authors provided four in their code. I think the loader for Federenko-2016 wouldn't work, and Pereira-2018 with its 150,000 FMRI values was going to take days to compute for. The same dubious `StratifiedKFold` is used for all four datasets though, so I would question the effect size of the results given in the paper. That said, the results could remain significant once this has been corrected for.\n\nAlso, this doesn't say anything about the many other papers out there that are doing similar things. For instance, Caucheteux et al (code unavailable) state that they use a `GroupKFold`, which sounds a lot better. They also present much lower brain_scores, though no explicit p-values for them, and no mention of how they addressed the multiple comparisons thing.\n\nIf I get some time, I might try to compute for the other datasets here. But for now, it seems inconclusive. Overall, I hope that gpt2-brain researchers can agree upon a reasonable way to do a train/test split. They could also do better at explaining to lay people what are acceptable underlying correlations and p-values for the normalized brain-score values that they present as evidence.\n\n\n[^1]: I had some problems installing this. In the end, what seemed to work was installing nltk, pytorch and tensorflow separately with pip/conda, grabbing the latest [brainscore](https://github.com/brain-score/brain-score/tree/master/brainscore) and [brainio_base](https://github.com/brain-score/brainio_base) packages directly from github, and only then running the neural_nlp setup.py script.\n[^2]: [https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184](https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184) This line in the brainscore code predates the release of the paper by at least 2 years, so it seems like they did in fact use it to compute their paper results.\n[^3]: In the dataset examined here, [Blank et al 2014]( https://journals.physiology.org/doi/full/10.1152/jn.00884.2013), there are only 60 values per time point. I assume this means that the values have been aggregated accross regions and participants, and restricted to only a set of voxels that are relevant for language processing. I'm not sure though. Anecdotally, I have heard that this FMRI thing can be something of a dark art.\n[^4]:See [1]\n[^5]: The funny thing is that anyone who downloaded and ran gpt2 back in 2019 will remember that it didn't really seem to understand language at all. \n\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.262","theme":"journal","title-block-banner":true,"title":"Does GPT2 correlate with your brain?","author":"dermot","date":"2022-11-16","categories":["code","analysis"],"image":"image2.webp","bibliography":["references.bib"]},"extensions":{"book":{"multiFile":true}}}}}