<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.262">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dermot">
<meta name="dcterms.date" content="2022-11-05">

<title>diarmuidmorgan@github.io - Does GPT2 correlate with your brain?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">diarmuidmorgan@github.io</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Does GPT2 correlate with your brain?</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">analysis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dermot </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 5, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="image2.webp" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Possibly the author on the left.</figcaption><p></p>
</figure>
</div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Large Language Models (LLMs) are doing really well for themselves right now. Not only have they crushed most existing language benchmarks into the dust, but recent iterations have seen success at everything from groking high school math questions, writing papers for overworked undergraduates, and even convincing google engineers to retain lawyers on their behalf. Debate as to the further scaling of these models often seems to point towards the dawn of AGI arriving just as soon as somebody stumps up the cash to wire up thousands of GPUs in parallel and feed them the entirety of the internet as training data.</p>
<p>What is perhaps most surprising is that all of the above seems to have emerged independent of any particular training goal. The networks aren’t optimized to write undergrad papers, or to exhibit convicing portrayals of embodied agency. They are tasked only with predicting the next most likely word in a sequence, given some previous context (e.g an incomplete sentence.) All the rest seems to appear ‘for free’ (or with a lot of clever prompting).</p>
<p>If this isn’t enough, a number of recent papers have claimed that existing LLMs actually predict human brain activity during language comprehension <span class="citation" data-cites="caucheteux2022deep schrimpf2021neural">(<a href="#ref-caucheteux2022deep" role="doc-biblioref">Caucheteux, Gramfort, and King 2022</a>; <a href="#ref-schrimpf2021neural" role="doc-biblioref">Schrimpf et al. 2021</a>)</span>. Despite never seeing a single blip of human brain activity, it seems like they can reliably correlate with the parts of the brain considered to be active during language processing. This might seem to be going a little far. LLMs are so amazing that they predict your brain too! With a healthy dose of skepticism, I’ll examine this claim by unpacking the source code of perhaps the best cited of these papers, Schrimpf et al 2021.</p>
</section>
<section id="schrimpf-et-als-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="schrimpf-et-als-pipeline">Schrimpf et al’s pipeline</h3>
<p>The Schrimpf et al paper tests a variety of pretrained LLM models on neural datasets, finding openAI’s gpt2 (which is the largest) to be the best. The authors follow a basic pipeline:</p>
<ol type="1">
<li><p>A dataset is chosen containing observed FMRI values for participants as they listen to recorded text. The dataset is arranged so that each group of FMRI values is matched with the sentence that was being played when the corresponding FMRI values were recorded<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p></li>
<li><p>The sentences are grouped into stories. The stories are passed into gpt2 (or a smaller LLM), one sentence at a time, but including the previous sentences in the story as context input, and the LLM’s activations are recorded at each layer.</p></li>
<li><p>Thus we have two datasets. One is a set of FMRI values paired with sentences, and the other is a set of gpt2 activations paired with the same sentences.</p></li>
<li><p>For each column of FMRI values, and for each layer of gpt2, they fit a Linear Regression model between gpt2’s activations at that layer, and the column of FMRI values. A train/test split is used.</p></li>
<li><p>Each of these models is evaluated on the pearson corrleation between the real and predicted FMRI values for the holdout test data.</p></li>
<li><p>For each layer, the correlations are aggregated across columns of FMRI values, and then normalized via a noise ceiling to produce a ‘brain score’ for that layer.</p></li>
</ol>
</section>
<section id="running-schrimpf-et-als-code-on-blank-2014" class="level3">
<h3 class="anchored" data-anchor-id="running-schrimpf-et-als-code-on-blank-2014">Running Schrimpf et al’s code on <a href="https://journals.physiology.org/doi/full/10.1152/jn.00884.2013">Blank 2014</a></h3>
<p>Schrimpf et al were kind enough to make their code available <a href="https://github.com/mschrimpf/neural-nlp">here</a>. I had some trouble getting it installed on my machine<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, but once its up and running the pipeline can be executed more or less like this:</p>
<pre><code>from neural_nlp import score as score_function
from neural_nlp.models import model_pool

model='gpt2-xl'
benchmark = 'Blank2014fROI-encoding'
layers=None
subsample=None


(raw, normalized )= score_function(model=model,
            layers=layers,
            subsample=subsample,
            benchmark=benchmark)


print(list([np.round(x,4) for x in raw.mean(axis=1).values]))

---&gt;[0.0063, 0.0063, 0.0237, 0.0249, 0.0378, 0.0342, 0.0228, 0.0241, 0.022, 0.0185, 0.0259,
 0.022, 0.0245, 0.0283, 0.03, 0.0409, 0.0424, 0.0373, 0.0422, 0.0425, 0.0447, 0.0374, 0.0437,
  0.0442, 0.0518, 0.0503, 0.0559, 0.0594, 0.0582, 0.0619, 0.0519, 0.0539, 0.0569, 0.0548, 0.0498,
   0.0468, 0.0504, 0.049, 0.0457, 0.0515, 0.0483, 0.0475, 0.0472, 0.0473, 0.0438, 0.041, 0.0407, 
   0.0385, 0.0366]

print([np.round(x,4) for x in normalized.values[:,0]])

---&gt;[0.0513, 0.0203, 0.1216, 0.1678, 0.2002, 0.1792, 0.1091, 0.1085, 0.1189, 0.0806, 0.1171,
 0.0688, 0.0999, 0.1618, 0.179, 0.1798, 0.2094, 0.1826, 0.1667, 0.2088, 0.2309, 0.1709,
  0.206, 0.2154, 0.265, 0.2758, 0.3062, 0.2916, 0.367, 0.4073, 0.3394, 0.3412, 0.3865, 0.3738,
   0.3075, 0.2637, 0.3211, 0.2725, 0.2605, 0.2515, 0.2349, 0.2225, 0.2003, 0.2031,
    0.2366, 0.2183, 0.2253, 0.1911, 0.1847]

----&gt;</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="run1.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">The output</figcaption><p></p>
</figure>
</div>
<p>After many hours of compute, we get the above results. The output is two instances of <code>DataAssembly</code>, the first one giving the raw correlations, and the second providing the final brain scores, normalized by a noise ceiling (a correlation upper bound that the authors precomputed). The normalized values give a score for each layer in gpt2. We can see that they’re at their strongest (max <code>0.4073</code>) in the middle to latter layers. So this seems ok. The raw correlations are really small (get back to this later), though Schrimpf et al assure us that this is quite good compared to the noise ceiling. But I’m not so easily convinced. Remember, that gpt2 has still never seen a single real life brain!</p>
</section>
<section id="signal-autocorrelation-and-a-random-embedding-trick." class="level3">
<h3 class="anchored" data-anchor-id="signal-autocorrelation-and-a-random-embedding-trick.">Signal Autocorrelation and a random embedding trick.</h3>
<p>So what if these FMRI signals were to correlate with themselves over time? For instance, your current heart beat is a reasonabley good indicator of your what heart beat will be ten seconds from now. Its going to do better than if I just make a blind guess, given no prior knowledge. Similarly, the FMRI signal value at one time point might not be a particularly bad predictor of the next timepoint. We can see how this works out for the FMRI data below.</p>
<pre><code>import numpy as np
from scipy.stats import pearsonr
benchmark=benchmark_pool['Blank2014fROI-encoding']
data = np.array(benchmark._target_assembly)
corrs=[]
skip=1

#for each FMRI column, lag the signal by one step and calculate correlation
#with the unlagged signal

for i in range(data.shape[1]):
    #lag the signal by 'skip' steps
    v=data[:,i]
    x=v[:-skip]
    y=v[skip:]
    cor = pearsonr(x,y)[0]
    corrs.append(cor)
print(np.mean(corrs))

---&gt; 0.41318839674224245</code></pre>
<p>And aha! They do. Given that, we might wonder if there is a way we could trick a LinearRegression on random data to predict the right signal. Well seemingly we can. Below, I initialize a random embedding at the beginning of each story and then proceed by injecting a small amount of noise at each time step. For each neuroid dimension, I then randomly split the data into 0.9/0.1 train/test and train a linear regression to predict the voxels from the random embeddings.</p>
<pre><code>from sklearn.linear_model import LinearRegression

#get the story name for each sample
stimuli_ids = data._target_assembly['stimulus_id'].values
stimuli_ids = [x.split('.')[0] for x in stim]

embed_dim=3000
embeds=[]
#make a random embedding
cur_embed = np.random.normal(0, 1, [embed_dim])
prev_name = 'A'
for stimuli_id in stimuli_ids:
    #if stimuli_id is a new story, create a new random embedding
    if prev_name != stimuli_id:
        cur_embed = np.random.normal(0, 1, [embed_dim])
    #otherwise, just add a (fairly large) amount of noise to the current embedding  
    else:
        cur_embed = cur_embed + np.random.normal(0, 0.1, [embed_dim])
    prev_name=stimuli_id
    embeds.append(cur_embed)
    
X=np.array(embeds)
corrs=[]

#for each neuroid, do a random train test split
#build a linear regression between the semi random embeddings and the target
for i in range(data.shape[1]):
    Y = data[:, i]
    train_idx = np.random.random(len(X))&lt;0.9
    train_x = X[train_idx]
    train_y = Y[train_idx].reshape(-1,1)
    test_x = X[~train_idx]
    test_y = Y [~train_idx].reshape(-1,1)
    m = LinearRegression().fit(train_x, train_y)
    pred = m.predict(test_x)
    cor,p = pearsonr(pred.flatten(), test_y.flatten())
    corrs.append(cor)

print('Mean correlation with simple train/test split', np.mean(corrs))

---&gt; Mean correlation with simple train/test split 0.41665761324952694</code></pre>
<p>So my simple embedding scheme effectively captures all of the autocorrelation implicit in the signals, and does much better than gpt2 in terms of raw correlation. We can bake this into Shcrimpf et al’s code with a quick hack:</p>
<pre><code>import neural_nlp
from neural_nlp import score as score_function
from neural_nlp.models import model_pool
import numpy as np
import itertools
from neural_nlp.utils import ordered_set
from brainio.assemblies import DataAssembly, walk_coords, merge_data_arrays, array_is_element
from neural_nlp.benchmarks.neural import Blank2014fROIEncoding

def listen_to(candidate, stimulus_set, reset_column='story', average_sentence=True):
    """
    Code is from neural_nlp.benchmarks.neural.listen_to
    Add code to replace story_activations with our random trick embeddings of the same size.
    
    Pass a stimulus_set through a model candidate.
    Operates on a sentence-based stimulus_set.
    """
    activations = []
    for story in ordered_set(stimulus_set[reset_column].values):
        story_stimuli = stimulus_set[stimulus_set[reset_column] == story]

        story_stimuli.name = f"{stimulus_set.name}-{story}"
        story_activations = candidate(stimuli=story_stimuli, average_sentence=average_sentence)
        
        #This is the only addition to the code - add our random trick in place of
        #gpt2's activations
        embed=np.random.normal(0,1 , [story_activations.shape[1]])
        new_activ = [embed]
        for i in range(1,story_activations.shape[0]):
            embed = embed + np.random.normal(0, 0.001, story_activations.shape[1])
            new_activ.append(embed)
        story_activations[:] = new_activ
        #End addition.
        
        
        activations.append(story_activations)
        
    model_activations = merge_data_arrays(activations)
    # merging does not maintain stimulus order. the following orders again
    idx = [model_activations['stimulus_id'].values.tolist().index(stimulus_id) for stimulus_id in
           itertools.chain.from_iterable(s['stimulus_id'].values for s in activations)]
    assert len(set(idx)) == len(idx), "Found duplicate indices to order activations"
    model_activations = model_activations[{'presentation': idx}]
    #the model activations returned here are actually ordered by story
   
    return model_activations


#need to change __call__ method to use the listen_to method above
class Blank2014RandomEmbed(Blank2014fROIEncoding):
    
    def __call__(self, candidate):

        model_activations = listen_to(candidate, self._target_assembly.attrs['stimulus_set'])
        assert set(model_activations['stimulus_id'].values) == set(self._target_assembly['stimulus_id'].values)
        score = self.apply_metric(model_activations, self._target_assembly).copy()
        
        normalized_score,raw_score = self.ceiling_normalize(score)
    

        return raw_score, normalized_score
    

benchmark_pool['Blank2014RandomEmbed'] = BlankRandomEmbed('Blank2014fROI-encoding')

model='gpt2-xl'
layers=None
subsample=None
benchmark='Blank2014RandomEmbed'

(raw, normalized )= score_function(model='gpt2-xl',
            layers=layers,
            subsample=subsample,
            benchmark=benchmark)
            
            
print([np.round(x,4) for x in normalized.values[:,0]])
----&gt;[1.6771, 1.7417, 1.5778, 1.7718, 1.6413, 1.6526, 1.6702, 1.6343,
 1.6861, 1.6316, 1.6659, 1.621, 1.7315, 1.7783, 1.7054, 1.6932, 1.5978,
  1.7578, 1.6671, 1.7184, 1.6518, 1.6154, 1.6411, 1.7644, 1.5474, 1.5287, 
  ]1.6716, 1.7877, 1.7792, 1.707, 1.7296, 1.6836, 1.7145, 1.7302, 1.7645, 
  1.7717, 1.568, 1.5788, 1.7494, 1.7074, 1.7392, 1.7495, 1.6844,
1.7248, 1.6814, 1.6247, 1.6582, 1.6814, 1.7328]</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="embeddingtrick.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Brain-scores for the random embeddings</figcaption><p></p>
</figure>
</div>
<p>The brain scores here are huge! They completely destroy those of gpt2! So what is going on? If my random embedding trick can do this well within the authors framework, then how can it be said to be adequately testing for anything at all?</p>
</section>
<section id="so-what-allowed-for-the-good-results" class="level3">
<h3 class="anchored" data-anchor-id="so-what-allowed-for-the-good-results">So what allowed for the ‘good’ results?</h3>
<p>My random embedding scheme relied upon the fact that each embedding in a story was ‘similar’ to its parent and child. I did this by injecting only a small amount of noise at each time step. If we think about it, the same thing should apply to gpt2’s activations. The learned representation of the sentence “The quick brown fox jumps over the lazy dog” should be ‘similar’ to “The quick brown fox jumps over the lazy dog and then goes in search of…”. After all, not an awful lot has really changed.</p>
<p>I tried to check this by calculating the 1-step cosine similarity values for the activations gpt2 produced - e.g the similarity between the embedding for “The quick brown fox jumps over the lazy dog” and that for “The quick brown fox jumps over the lazy dog and then goes in search of…” . This didn’t work as expected. The lower (worst performing) layers of gpt2 had the best interstep similarity, which is contrary to what I would have expected from the results. So who knows - maybe cosine similarity just isn’t a useful measure for LLM embeddings.</p>
<p>What I was able to do though, was show that gpt2’s layer activations would exhibit similar behaviour when tested against random autocorrelated signals that I generated off the bat:</p>
<pre><code>from neural_nlp.benchmarks import benchmark_pool
from scipy.stats import pearsonr
from sklearn.linear_model import LinearRegression
import numpy as np
import pickle

#Thanks to https://stackoverflow.com/a/33904277 for the sample_signal code
def sample_signal(n_samples, corr, mu=0, sigma=1):
    assert 0 &lt; corr &lt; 1, "Auto-correlation must be between 0 and 1"

    # Find out the offset `c` and the std of the white noise `sigma_e`
    # that produce a signal with the desired mean and variance.
    # See https://en.wikipedia.org/wiki/Autoregressive_model
    # under section "Example: An AR(1) process".
    c = mu * (1 - corr)
    sigma_e = np.sqrt((sigma ** 2) * (1 - corr ** 2))

    # Sample the auto-regressive process.
    signal = [c + np.random.normal(0, sigma_e)]
    for _ in range(1, n_samples):
        signal.append(c + corr * signal[-1] + np.random.normal(0, sigma_e))

    return np.array(signal)

def compute_corr_lag_1(signal):
    return np.corrcoef(signal[:-1], signal[1:])[0][1]


def load(f):
    
    with open(f, 'rb') as handle:
      
        return pickle.load(handle)



layer_scores=[]
#create a random autocorrelated signal in the place of each neuroid
signal=[ sample_signal(len(data),0.5) for j in range(50)]

for i in range(1, 50):
    
    #I saved the layer activations to disk.
    activations = load('activations/{}.bin'.format(str(i))).values
    
    corrs=[]
    for j in range(50):

        #80/20 train/test plit
        train_idx = np.random.random(data.shape[0])&lt;0.8 
        test_idx = ~train_idx
        train_X = activations[train_idx]
        train_Y = signal[j].reshape(-1,1)[train_idx]
        test_X = activations[test_idx]
        test_Y = signal[j].reshape(-1,1)[test_idx]

        #fit model between gpt2 and the random signal
        model = LinearRegression().fit(train_X, train_Y)
        #predict and evaluate
        pred = model.predict(test_X)
        cor, p = pearsonr(pred.flatten(), test_Y.flatten())
        corrs.append(cor)
        
  
    layer_scores.append(np.mean(corrs))
        
print([np.round(f,4) for f in list(layer_scores)]) 

---&gt; [0.0032, 0.0014, 0.0198, 0.0264, 0.034, 0.0213, 0.0246, 0.0213, 0.0306,
 0.03, 0.018, 0.0069, 0.0136, 0.0294, 0.0184, 0.0232, 0.025, 0.0365, 0.0338, 
 0.0464, 0.0588, 0.0526, 0.0419, 0.0442, 0.0619, 0.0636, 0.0553, 0.0373,
  0.0368, 0.0485, 0.0614, 0.0494, 0.0541, 0.0519, 0.0507, 0.0511, 0.0563, 
  0.0651, 0.0482, 0.049, 0.0468, 0.0589, 0.0565, 0.0656, 0.0531, 0.0634, 
  0.0515, 0.0704, 0.0357]</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="randomsignal.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Gpt2 correlation with random signal</figcaption><p></p>
</figure>
</div>
<p>The raw correlation scores get better as you go deeper into the model, and then decrease towards the end. This doesn’t happen quite as smoothly as when fitting gpt2 to the FMRI data. But there you go. Not only can a set of random embeddings excel within the paper’s pipeline, but gpt2 can ‘predict’ more or less any autocorrelated signal, when tested in this way.</p>
</section>
<section id="examining-the-traintest-split." class="level3">
<h3 class="anchored" data-anchor-id="examining-the-traintest-split.">Examining the train/test split.</h3>
<p>The discerning reader noticed that I’ve been performing random train/test splits, with no regards as to the ‘series’ nature of the data. And of course, the authors do this <a href="https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184">too</a>. The option that the authors default to is:</p>
<p><code>self._split = StratifiedKFold(n_splits=splits, shuffle=True, random_state=random_state)</code><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>But this is definitely not how you’re meant to handle time series data. The use of <code>shuffle=True</code> means that the voxel values from the end of each story are mixed up with those at the beginning, and the test values are just randomly missing points in the time series.</p>
<p>If we wanted to just use a consecutive K-Fold cross validation, we could fix it like this:</p>
<pre><code>...
self._split = KFold(n_splits=splits, shuffle=False)
...</code></pre>
<p>Recomputing is quick, as the previously calculated gpt2 activations have been stored.</p>
<pre><code>...
...
print(list([np.round(x,4) for x in raw.mean(axis=1).values]))
---&gt;[0.0099, 0.0065, 0.003, -0.0029, 0.0054, 0.0101, 0.0051, 0.0047, -0.0082, 0.0025,
 0.0047, 0.0088, 0.008, 0.0067, 0.0053, 0.0134, 0.0086, 0.0041, 0.0133,
  0.0174, 0.0131, 0.0053, 0.0045, 0.0043, 0.0075, 0.0127, 0.0147, 0.0166,
   0.0142, 0.0169, 0.0151, 0.0124, 0.0179, 0.0143, 0.0076, 0.0002, -0.0007, 
   -0.0005, -0.0002, 0.0005, -0.0019, -0.0001, -0.0018, 0.0008, 0.0022, -0.0002,
    0.0029, 0.0018, 0.0042]

print([np.round(x,4) for x in normalized.values[:,0]])
---&gt;[0.0486, 0.0586, 0.0251, -0.017, 0.075, 0.0555, 0.0691,
 0.0358, -0.041, 0.0167, 0.0174, 0.0582, 0.0196, 0.0136,
  0.0446, 0.0415, 0.0772, 0.063, 0.0271, 0.1104, 0.0601,
   0.013, 0.0308, -0.031, 0.0151, 0.0978, 0.0963, 0.1133,
    0.1328, 0.1853, 0.1706, 0.0971, 0.1552, 0.1403, 0.023, 
    0.0135, 0.0109, -0.0029, -0.0107, -0.0227, -0.0169,
     0.0412, 0.0203, 0.0424, -0.0042, -0.0253, 0.0212, 0.0083, 0.0184]</code></pre>
<p>Those scores are really different. But I think there is still some possibility of overlap occurring - future values being used as training data for test values earlier in the series. Another option is to do away with cross validation and just use a single split, using the first 80% of a each story as training data, and the last 20% as test. Thus we ensure that no future data is seen by the linear regressions. This is assuming that the stories constitute breaks in the time series. It’s hacky, but we pop this change into the brain_score code <a href="https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L311">here</a> so that we run the cross validation 5 times on the same split.</p>
<pre><code>...
...
...
train_idx = []
test_idx=[]
from collections import defaultdict
d=defaultdict(list)
stimulus_ids = target_assembly['stimulus_id'].values
for idx, stimulus_id in enumerate(stimulus_ids):

    stimulus_id=stimulus_id.split('.')[0]
    d[stimulus_id].append(idx)

for stimulus_id in d:
    idxs=d[stimulus_id]
    cutoff=int(len(idxs)*0.8)
    train_idx.extend(list(idxs[:cutoff]))
    test_idx.extend(list(idxs[cutoff:]))

for split_iterator, (train_indices, test_indices), done \
                in tqdm(enumerate_done(splits), total=len(splits), desc='cross-validation'):
            
    train_indices,test_indices = train_idx, test_idx

...
...
...

print(list([np.round(x,4) for x in raw.mean(axis=1).values]))
---&gt;[0.0022, 0.0179, 0.008, 0.0018, 0.0106, 0.019, 0.0127, 0.0126, 0.0002,
 0.0037, 0.0047, 0.0111, 0.0089, 0.0147, 0.0069, 0.0161, 0.0131, 0.0064, 
 0.0164, 0.0191, 0.015, 0.0075, 0.0083, 0.0086, 0.0142, 0.0172, 0.0207, 
 0.0222, 0.0207, 0.0226, 0.0204, 0.0202, 0.0269, 0.023, 0.0183, 0.0088, 
 0.0094, 0.0092, 0.0062, 0.0065, 0.0043, 0.0041, -0.0002, 0.0023, 0.0063, 
 0.0045, 0.005, 0.0034, 0.0083]

print([np.round(x,4) for x in normalized.values[:,0]])
---&gt;[0.0268, -0.0866, 0.1223, -0.0465, -0.1973, 0.0199, 
-0.0719, 0.0215, -0.0109, -0.1299, 0.083, -0.0626, -0.0418, 
-0.0597, 0.0077, 0.0426, -0.0827, 0.0612, 0.0772, 0.1293, 
0.0654, -0.0816, 0.0846, -0.017, 0.1064, 0.0623, 0.0701, 
0.046, 0.1553, 0.1693, 0.0432, 0.1265, 0.008, 0.0444, 0.0082, 
-0.0185, -0.0137, -0.0178, 0.0317, 0.0407, -0.0013, 0.0121, 
-0.0307, 0.0045, 0.0989, -0.1037, -0.0928, -0.153, -0.0165]</code></pre>
<p>That’s a big difference. The brain scores have pretty much collapsed, and there are a load of negative scores now. Alternatively, we could consider the whole dataset as one consecutive time series, and train on the first 80% of values and test only on the final 20%. E.g:</p>
<pre><code>...
...
idx = np.arange(target_assembly.shape[0])
cut_off = int(len(idx)*0.8)
train_idx = idx[:cut_off]
test_idx = idx[cut_off:]
...
...

print(list([np.round(x,4) for x in raw.mean(axis=1).values]))
---&gt;[0.0014, 0.0081, 0.0092, -0.001, 0.0131, 0.0026, -0.0079, 0.0155, -0.0059, 
0.0062, 0.0066, 0.0328, 0.0234, 0.0069, 0.0055, 0.0235, 0.0118, 0.0146, 0.0236, 
0.0262, 0.0165, 0.0062, 0.0096, 0.0086, 0.0072, 0.0194, 0.0251, 0.0239, 0.0125, 
0.0108, 0.0156, 0.0142, 0.0334, 0.0216, 0.0056, -0.0068, -0.0012, 0.001, 0.0057, 
0.0014, 0.0007, -0.0181, -0.0114, -0.0016, -0.0023, -0.0104, -0.009, -0.0104, -0.002]

print([np.round(x,4) for x in normalized.values[:,0]])
---&gt;[-0.0213, 0.0667, -0.0945, 0.0035, 0.0844, -0.0822, -0.0548, 0.18, 
0.0561, 0.0135, 0.0509, 0.2229, 0.0795, 0.0935, 0.0614, 0.1577, 0.0676,
 0.1072, 0.1658, 0.1828, 0.0529, 0.0734, 0.0523, 0.0425, 0.0354, 0.126, 
 0.1824, 0.0505, 0.0899, 0.0414, 0.0949, 0.124, 0.2261, 0.2791, 0.1468, 0.0557, 
 0.0382, 0.0193, 0.0594, 0.0374, 0.0412, -0.02, -0.0065, -0.0035, 0.0318, -0.0479, -0.0127, 0.0144, 0.0567]
</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="finalcomparison.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Brain score comparisons across split strategies</figcaption><p></p>
</figure>
</div>
<p>As the gpt2 activations have been saved, it all recomputes pretty quickly, and we get the above results. The mean normalized brain_score across layers in the first instance was <code>0.21432</code>. For the KFold fix, it went down to <code>0.044738</code>. Splitting the stories gave us <code>0.008828</code>, and just naively splitting the whole dataset 80/20 gives us <code>0.06575102040816326</code>. And look at the raw correlation values! There are still one or two layers showing some layer/brain correlation, but the values are tiny. But, a correlation is still a correlation. Or is it?</p>
</section>
<section id="what-makes-a-significant-brain-score" class="level3">
<h3 class="anchored" data-anchor-id="what-makes-a-significant-brain-score">What makes a significant brain-score?</h3>
<p>I wanted to look a bit further, so I unpacked the actual p-values for the correlations that the best scoring layer gpt2 produced.</p>
<pre><code>activations=load('activations/{}.bin'.format(str(best_layer+1))).values
idx = np.arange(data.shape[0])
cut_off = int(len(idx)*0.8)
train_idx=idx[:cut_off]
test_idx=idx[cut_off:]
train_X = activations[train_idx]
train_Y = data.values[train_idx]
test_X = activations[test_idx]
test_Y = data.values[test_idx]
model = LinearRegression().fit(train_X, train_Y)
pred = model.predict(test_X)
ps=[]

for j in range(data.shape[1]):

    cor, p = pearsonr(pred[:,j].flatten(), test_Y[:,j].flatten())
    ps.append(p)

print(ps)
print(np.mean(ps))

----&gt;[0.1261, 0.4546, 0.7403, 0.4028, 0.1986, 0.8898, 0.2407,
 0.5314, 0.4364, 0.9006, 0.8181, 0.4686, 0.0237, 0.3228, 0.7242,
  0.3558, 0.0549, 0.1904, 0.9157, 0.3441, 0.1256, 0.0802, 0.5643,
   0.4628, 0.1521, 0.9165, 0.2372, 0.6676, 0.9878, 0.9566, 0.6633,
    0.1939, 0.2056, 0.6876, 0.7831, 0.2193, 0.9157, 0.0956, 0.8299,
     0.867, 0.4778, 0.8489, 0.6799, 0.3197, 0.7199, 0.7147, 0.6371, 
     0.8556, 0.374, 0.103, 0.8697, 0.2803, 0.4908, 0.0162, 0.7662,
      0.3224, 0.6696, 0.0839, 0.0171, 0.9656]</code></pre>
<p>So they’re really high. Really, really high. If the significance threshold is a conservative 0.05, only 3 voxels actually test out below that. Most of them look like garbage.</p>
<p>But what do we do with these p-values? How can we judge the significance of the overall brain-score (average correlation).</p>
<p>A straight up approach might suggest we’re doing multiple comparisons e.g testing multiple different hypotheses. If we follow the <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a> method, then we would end up setting our significance threshold 60 times lower, and reject everything. But that can’t be how people generally treat brain voxels right?</p>
<p>As per wikipedia, multiple comparisons assumes that we are testing quite different hypotheses e.g a new teaching method improves spelling, arithmetic, reading comprehension. But these voxels, they’re kind of like similar hypotheses right? So perhaps the p-value for the final brain score should end up lower?</p>
<p>Anyway I couldn’t decide. I thought maybe we could empirically estimate p-values for the brain-scores by running the test thousands of times but with randomly generated gpt2 activations. Then we can see an approximate likelihood of doing as well as gpt2 with completely uncorrelated data. It’s a bit rough and ready, but I guess it will do in the absence of something better.</p>
<pre><code>brain_scores = []
for i in range(1000):
    
    idx = np.arange(data.shape[0])
    cut_off = int(len(idx)*0.8)
    train_idx=idx[:cut_off]
    test_idx=idx[cut_off:]
    activations = np.random.normal(0, 0.1, activations.shape)
    train_X = activations[train_idx]
    train_Y = data.values[train_idx]

    test_X = activations[test_idx]
    test_Y = data.values[test_idx]
    model = LinearRegression().fit(train_X, train_Y)
    pred = model.predict(test_X)
    corrs=[]
    ps=[]
  
    for j in range(data.shape[1]):

        cor, p = pearsonr(pred[:,j].flatten(), test_Y[:,j].flatten())
        corrs.append(cor)
    
        
   
    brain_scores.append(np.mean(corrs))

sorted_brain_scores=sorted(brain_scores)
sorted_brain_scores=np.flip(sorted_brain_scores)[:200]
x=np.arange(len(sorted_brain_scores))*0.001
%matplotlib inline
from matplotlib import pyplot as plt
plt.plot(x, sorted_brain_scores)
plt.xlabel('p-value')
plt.ylabel('correlation')
plt.show()</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="pvalues2.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Average-Correlation/p-value plot</figcaption><p></p>
</figure>
</div>
<p>So if that’s right, an average_correlation at or above <code>0.0195</code> is ‘significant’, at <code>p&lt;0.05</code>. That puts several layers of gpt2 well within significance territory.</p>
<p>Still, doesn’t testing 50 different gpt2 activation layers sound an awful lot like testing 50 different hypothesis? If so, then applying Bonferroni correction we’d be adjusting the significance threshold down to <code>p&lt;=0.001</code>, or an average correlation of <code>0.0412</code>. That way we’d once again end up rejecting everything.</p>
<p>I can’t decide. I guess gpt2 might correlate with your brain, but only very, very slightly.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>Altogether, this paper hasn’t convinced me that gpt2 knows anything about my brain. There are some caveats though. As mentioned<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, I couldn’t get the package to install cleanly, so I would be a little wary of package mismatches. Also, I have only tested one dataset, whereas the authors provided four in their code. I think the loader for Federenko-2016 wouldn’t work, and Pereira-2018 with its 150,000 FMRI values was going to take days to compute for. The same dubious <code>StratifiedKFold</code> is used for all four datasets though, so I would question the effect size of the results given in the paper. That said, the results could remain significant once this has been corrected for.</p>
<p>The choice of train/test split in the code seems like an obvious mistake - gpt2 can seemingly correlate with any random signal if you use the given approach. But I was really confused about the difference in value given from splitting by story vs a clean 80/20 consecutive split. Unfortunately, this kind of prevents me from tying everything together nicely.</p>
<p>Lastly, this doesn’t say anything about the many other papers out there that are doing similar things. For instance, Caucheteux et al (code unavailable) state that they use a GroupKFold, which might well be better. They also present much lower brain_scores, though no explicit p-values for them, and no mention of how they addressed the multiple comparisons thing.</p>
<p>If I get some time, I might try to compute the other datasets here. But for now, it seems inconclusive. I guess that gp2 might correlate with my brain, but only very, very slightly?</p>
<p>Overall, I hope that gpt2-brain researchers can agree upon a reasonable way to do a train/test split, and also to do better at explaining to lay people what are acceptable underlying correlations and p-values for the normalized brain-score values that they present as evidence.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-caucheteux2022deep" class="csl-entry" role="doc-biblioentry">
Caucheteux, Charlotte, Alexandre Gramfort, and Jean-Rémi King. 2022. <span>“Deep Language Algorithms Predict Semantic Comprehension from Brain Activity.”</span> <em>Scientific Reports</em> 12 (1): 1–10.
</div>
<div id="ref-schrimpf2021neural" class="csl-entry" role="doc-biblioentry">
Schrimpf, Martin, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. 2021. <span>“The Neural Architecture of Language: Integrative Modeling Converges on Predictive Processing.”</span> <em>Proceedings of the National Academy of Sciences</em> 118 (45): e2105646118.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>In the dataset examined here, <a href="https://journals.physiology.org/doi/full/10.1152/jn.00884.2013">Blank et al 2014</a>, there are only 60 values per time point. I assume this means that the values have been aggregated accross regions and participants, and restricted to only a set of voxels that are relevant for language processing. I’m not sure though. Anecdotally, I have heard that this FMRI thing can be something of a dark art.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I had some problems installing this. In the end, what seemed to work was installing nltk, pytorch and tensorflow separately with pip/conda, grabbing the latest <a href="https://github.com/brain-score/brain-score/tree/master/brainscore">brainscore</a> and <a href="https://github.com/brain-score/brainio_base">brainio_base</a> packages directly from github, and only then running the neural_nlp setup.py script.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><a href="https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184">https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184</a> This line in the brainscore code predates the release of the paper by at least 2 years, so it seems like they did in fact use it to compute their paper results.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>I had some problems installing this. In the end, what seemed to work was installing nltk, pytorch and tensorflow separately with pip/conda, grabbing the latest <a href="https://github.com/brain-score/brain-score/tree/master/brainscore">brainscore</a> and <a href="https://github.com/brain-score/brainio_base">brainio_base</a> packages directly from github, and only then running the neural_nlp setup.py script.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>