<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.262">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="dermot">
<meta name="dcterms.date" content="2022-11-16">

<title>diarmuidmorgan@github.io - Does GPT2 correlate with your brain?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">diarmuidmorgan@github.io</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Does GPT2 correlate with your brain?</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">code</div>
                <div class="quarto-category">analysis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>dermot </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 16, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="image2.webp" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Possibly the author on the left.</figcaption><p></p>
</figure>
</div>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>Large Language Models (LLMs) have had quite a run over the past few years. Not only have they crushed most existing language benchmarks into the dust, but recent iterations have excelled at everything from groking high school math questions and writing papers for overworked undergraduates, to even convincing google engineers to retain lawyers on their behalf. With further scaling, it sometimes seems that full AGI will be arriving just as soon as somebody stumps up the cash to wire just enough GPUs in parallel and feed them the entirety of the internet as training data.</p>
<p>Perhaps surprisingly, much of the above has emerged in the absence of any particular training goal. The networks aren’t optimized to write coherent undergrad papers, or to exhibit convicing portrayals of embodied agency. They are tasked only with predicting the next most likely word in a sequence, given some previous context (e.g an incomplete sentence). All the rest seems to appear ‘for free’ (or with a lot of clever prompting).</p>
<p>If this isn’t enough, a number of recent papers have claimed that existing LLMs actually predict human brain activity during language comprehension <span class="citation" data-cites="caucheteux2022deep schrimpf2021neural">(<a href="#ref-caucheteux2022deep" role="doc-biblioref">Caucheteux, Gramfort, and King 2022</a>; <a href="#ref-schrimpf2021neural" role="doc-biblioref">Schrimpf et al. 2021</a>)</span>. Despite never seeing a single blip of human neurology, it seems that their internal activations reliably correlate with the parts of the brain considered to be active during language processing. This might seem to be taking things a little too far - LLMs are so amazing that they can even predict your brain! With a healthy dose of skepticism, I’ll examine this claim by unpacking the source code of perhaps the best cited of these papers, Schrimpf et al 2021.</p>
</section>
<section id="schrimpf-et-als-pipeline" class="level3">
<h3 class="anchored" data-anchor-id="schrimpf-et-als-pipeline">Schrimpf et al’s pipeline</h3>
<p>The Schrimpf et al paper tests a variety of pretrained LLM models on neural datasets, finding openAI’s gpt2 (which is the largest)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> to be the best. The authors follow a basic pipeline:</p>
<ol type="1">
<li><p>A dataset is chosen containing observed FMRI values for participants as they listen to recorded text. The dataset is arranged so that each group of FMRI values is matched with the sentence that was being played when the corresponding FMRI values were recorded<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p></li>
<li><p>The sentences are grouped into stories. The stories are passed into gpt2 (or a smaller LLM), one sentence at a time, but including the previous sentences in the story as contextual input, and the LLM’s activations are recorded at each layer.</p></li>
<li><p>Thus we have two datasets. One is a set of FMRI values paired with sentences, and the other is a set of gpt2 activations paired with the same sentences.</p></li>
<li><p>For each column of FMRI values, and for each layer of gpt2, they fit a Linear Regression model between gpt2’s activations at that layer, and the column of FMRI values. A train/test split is used.</p></li>
<li><p>Each of these models is evaluated on the pearson corrleation between the real and predicted FMRI values for the holdout test data.</p></li>
<li><p>For each layer, the correlations are aggregated across columns of FMRI values, and then normalized via a noise ceiling to produce a ‘brain score’ for that layer.</p></li>
</ol>
</section>
<section id="running-schrimpf-et-als-code-on-blank-2014" class="level3">
<h3 class="anchored" data-anchor-id="running-schrimpf-et-als-code-on-blank-2014">Running Schrimpf et al’s code on <a href="https://journals.physiology.org/doi/full/10.1152/jn.00884.2013">Blank 2014</a></h3>
<p>Schrimpf et al are kind enough to make their code available <a href="https://github.com/mschrimpf/neural-nlp">here</a>. I had some trouble getting it installed on my machine<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, but once it’s up and running the pipeline can be executed more or less like this:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> neural_nlp <span class="im">import</span> score <span class="im">as</span> score_function</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> neural_nlp.models <span class="im">import</span> model_pool</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>model<span class="op">=</span><span class="st">'gpt2-xl'</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>benchmark <span class="op">=</span> <span class="st">'Blank2014fROI-encoding'</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>layers<span class="op">=</span><span class="va">None</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>subsample<span class="op">=</span><span class="va">None</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>(raw, normalized )<span class="op">=</span> score_function(model<span class="op">=</span>model,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            layers<span class="op">=</span>layers,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            subsample<span class="op">=</span>subsample,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>            benchmark<span class="op">=</span>benchmark)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> raw.mean(axis<span class="op">=</span><span class="dv">1</span>).values]))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span>[<span class="fl">0.0063</span>, <span class="fl">0.0063</span>, <span class="fl">0.0237</span>, <span class="fl">0.0249</span>, <span class="fl">0.0378</span>, <span class="fl">0.0342</span>, <span class="fl">0.0228</span>, <span class="fl">0.0241</span>, <span class="fl">0.022</span>, <span class="fl">0.0185</span>, <span class="fl">0.0259</span>,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.022</span>, <span class="fl">0.0245</span>, <span class="fl">0.0283</span>, <span class="fl">0.03</span>, <span class="fl">0.0409</span>, <span class="fl">0.0424</span>, <span class="fl">0.0373</span>, <span class="fl">0.0422</span>, <span class="fl">0.0425</span>, <span class="fl">0.0447</span>, <span class="fl">0.0374</span>, <span class="fl">0.0437</span>,</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0442</span>, <span class="fl">0.0518</span>, <span class="fl">0.0503</span>, <span class="fl">0.0559</span>, <span class="fl">0.0594</span>, <span class="fl">0.0582</span>, <span class="fl">0.0619</span>, <span class="fl">0.0519</span>, <span class="fl">0.0539</span>, <span class="fl">0.0569</span>, <span class="fl">0.0548</span>, <span class="fl">0.0498</span>,</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.0468</span>, <span class="fl">0.0504</span>, <span class="fl">0.049</span>, <span class="fl">0.0457</span>, <span class="fl">0.0515</span>, <span class="fl">0.0483</span>, <span class="fl">0.0475</span>, <span class="fl">0.0472</span>, <span class="fl">0.0473</span>, <span class="fl">0.0438</span>, <span class="fl">0.041</span>, <span class="fl">0.0407</span>, </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.0385</span>, <span class="fl">0.0366</span>]</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> normalized.values[:,<span class="dv">0</span>]])</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span>[<span class="fl">0.0513</span>, <span class="fl">0.0203</span>, <span class="fl">0.1216</span>, <span class="fl">0.1678</span>, <span class="fl">0.2002</span>, <span class="fl">0.1792</span>, <span class="fl">0.1091</span>, <span class="fl">0.1085</span>, <span class="fl">0.1189</span>, <span class="fl">0.0806</span>, <span class="fl">0.1171</span>,</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0688</span>, <span class="fl">0.0999</span>, <span class="fl">0.1618</span>, <span class="fl">0.179</span>, <span class="fl">0.1798</span>, <span class="fl">0.2094</span>, <span class="fl">0.1826</span>, <span class="fl">0.1667</span>, <span class="fl">0.2088</span>, <span class="fl">0.2309</span>, <span class="fl">0.1709</span>,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.206</span>, <span class="fl">0.2154</span>, <span class="fl">0.265</span>, <span class="fl">0.2758</span>, <span class="fl">0.3062</span>, <span class="fl">0.2916</span>, <span class="fl">0.367</span>, <span class="fl">0.4073</span>, <span class="fl">0.3394</span>, <span class="fl">0.3412</span>, <span class="fl">0.3865</span>, <span class="fl">0.3738</span>,</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.3075</span>, <span class="fl">0.2637</span>, <span class="fl">0.3211</span>, <span class="fl">0.2725</span>, <span class="fl">0.2605</span>, <span class="fl">0.2515</span>, <span class="fl">0.2349</span>, <span class="fl">0.2225</span>, <span class="fl">0.2003</span>, <span class="fl">0.2031</span>,</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.2366</span>, <span class="fl">0.2183</span>, <span class="fl">0.2253</span>, <span class="fl">0.1911</span>, <span class="fl">0.1847</span>]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="op">----&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="run1.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">The output</figcaption><p></p>
</figure>
</div>
<p>After many hours of compute, we get the above results. The output is two instances of <code>DataAssembly</code>, the first one giving the raw correlations, and the second providing the final brain scores. We can see that they’re at their strongest (max <code>0.4073</code>) in the middle to latter layers of gpt2. So this seems ok. The raw correlations are really small (get back to this later), though Schrimpf et al assure us that this is quite good compared to the noise ceiling. But I’m not so easily convinced. Remember, that gpt2 has still never seen a single real life brain!</p>
</section>
<section id="signal-autocorrelation-and-a-random-embedding-trick." class="level3">
<h3 class="anchored" data-anchor-id="signal-autocorrelation-and-a-random-embedding-trick.">Signal Autocorrelation and a random embedding trick.</h3>
<p>What if these FMRI signals were to correlate with themselves over time? For instance, your current heart beat is a reasonably good indicator of what your heart beat will be ten seconds from now. Its going to do better than if I just make a blind guess, given no prior knowledge. Similarly, the FMRI signal value at one time point might not be a particularly bad predictor of the next timepoint. We can see how this works out for the FMRI data below.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> pearsonr</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>benchmark<span class="op">=</span>benchmark_pool[<span class="st">'Blank2014fROI-encoding'</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(benchmark._target_assembly)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>corrs<span class="op">=</span>[]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>skip<span class="op">=</span><span class="dv">1</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">#for each FMRI column, lag the signal by one step and calculate correlation</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">#with the unlagged signal</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(data.shape[<span class="dv">1</span>]):</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#lag the signal by 'skip' steps</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    v<span class="op">=</span>data[:,i]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span>v[:<span class="op">-</span>skip]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span>v[skip:]</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    cor <span class="op">=</span> pearsonr(x,y)[<span class="dv">0</span>]</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    corrs.append(cor)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.mean(corrs))</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span> <span class="fl">0.41318839674224245</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>They do. Given that, we might wonder if there is a way we could trick a linear regression on random data to predict the right signal. Well seemingly we can. Below, I initialize a random embedding at the beginning of each story and then proceed by injecting a small amount of noise at each time step. For each voxel dimension, I then randomly split the data into 0.9/0.1 train/test and train a linear regression to predict the voxels from the random embeddings.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#get the story name for each sample</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>stimuli_ids <span class="op">=</span> data._target_assembly[<span class="st">'stimulus_id'</span>].values</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>stimuli_ids <span class="op">=</span> [x.split(<span class="st">'.'</span>)[<span class="dv">0</span>] <span class="cf">for</span> x <span class="kw">in</span> stimuli_ids]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>embed_dim<span class="op">=</span><span class="dv">3000</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>embeds<span class="op">=</span>[]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#make a random embedding</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>cur_embed <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, [embed_dim])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>prev_name <span class="op">=</span> <span class="st">'A'</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> stimuli_id <span class="kw">in</span> stimuli_ids:</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#if stimuli_id is a new story, create a new random embedding</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> prev_name <span class="op">!=</span> stimuli_id:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        cur_embed <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, [embed_dim])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">#otherwise, just add a (fairly large) amount of noise to the current embedding  </span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        cur_embed <span class="op">=</span> cur_embed <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, [embed_dim])</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    prev_name<span class="op">=</span>stimuli_id</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    embeds.append(cur_embed)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>X<span class="op">=</span>np.array(embeds)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>corrs<span class="op">=</span>[]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co">#for each voxel, do a random train test split</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co">#build a linear regression between the semi random embeddings and the target</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(data.shape[<span class="dv">1</span>]):</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> data[:, i]</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    train_idx <span class="op">=</span> np.random.random(<span class="bu">len</span>(X))<span class="op">&lt;</span><span class="fl">0.9</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> X[train_idx]</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    train_y <span class="op">=</span> Y[train_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    test_x <span class="op">=</span> X[<span class="op">~</span>train_idx]</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    test_y <span class="op">=</span> Y [<span class="op">~</span>train_idx].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> LinearRegression().fit(train_x, train_y)</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> m.predict(test_x)</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    cor,p <span class="op">=</span> pearsonr(pred.flatten(), test_y.flatten())</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    corrs.append(cor)</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Mean correlation with simple train/test split'</span>, np.mean(corrs))</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span> Mean correlation <span class="cf">with</span> simple train<span class="op">/</span>test split <span class="fl">0.41665761324952694</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So my simple embedding scheme effectively captures all of the autocorrelation implicit in the signals, and does much better than gpt2 in terms of raw correlation. We can bake this into Shcrimpf et al’s code with a quick hack:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> neural_nlp</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> neural_nlp <span class="im">import</span> score <span class="im">as</span> score_function</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> neural_nlp.models <span class="im">import</span> model_pool</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> neural_nlp.utils <span class="im">import</span> ordered_set</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> brainio.assemblies <span class="im">import</span> DataAssembly, walk_coords, merge_data_arrays, array_is_element</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> neural_nlp.benchmarks.neural <span class="im">import</span> Blank2014fROIEncoding</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> listen_to(candidate, stimulus_set, reset_column<span class="op">=</span><span class="st">'story'</span>, average_sentence<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">    Code is from neural_nlp.benchmarks.neural.listen_to</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">    Add code to replace story_activations with our random trick embeddings of the same size.</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Pass a stimulus_set through a model candidate.</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Operates on a sentence-based stimulus_set.</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    activations <span class="op">=</span> []</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> story <span class="kw">in</span> ordered_set(stimulus_set[reset_column].values):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>        story_stimuli <span class="op">=</span> stimulus_set[stimulus_set[reset_column] <span class="op">==</span> story]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        story_stimuli.name <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>stimulus_set<span class="sc">.</span>name<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>story<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        story_activations <span class="op">=</span> candidate(stimuli<span class="op">=</span>story_stimuli, average_sentence<span class="op">=</span>average_sentence)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">#This is the only addition to the code - add our random trick in place of</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="co">#gpt2's activations</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        embed<span class="op">=</span>np.random.normal(<span class="dv">0</span>,<span class="dv">1</span> , [story_activations.shape[<span class="dv">1</span>]])</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        new_activ <span class="op">=</span> [embed]</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,story_activations.shape[<span class="dv">0</span>]):</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>            embed <span class="op">=</span> embed <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.001</span>, story_activations.shape[<span class="dv">1</span>])</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            new_activ.append(embed)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        story_activations[:] <span class="op">=</span> new_activ</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>        <span class="co">#End addition.</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        activations.append(story_activations)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>    model_activations <span class="op">=</span> merge_data_arrays(activations)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># merging does not maintain stimulus order. the following orders again</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> [model_activations[<span class="st">'stimulus_id'</span>].values.tolist().index(stimulus_id) <span class="cf">for</span> stimulus_id <span class="kw">in</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>           itertools.chain.from_iterable(s[<span class="st">'stimulus_id'</span>].values <span class="cf">for</span> s <span class="kw">in</span> activations)]</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(<span class="bu">set</span>(idx)) <span class="op">==</span> <span class="bu">len</span>(idx), <span class="st">"Found duplicate indices to order activations"</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>    model_activations <span class="op">=</span> model_activations[{<span class="st">'presentation'</span>: idx}]</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">#the model activations returned here are actually ordered by story</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model_activations</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="co">#need to change __call__ method to use the listen_to method above</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Blank2014RandomEmbed(Blank2014fROIEncoding):</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, candidate):</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>        model_activations <span class="op">=</span> listen_to(candidate, <span class="va">self</span>._target_assembly.attrs[<span class="st">'stimulus_set'</span>])</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">set</span>(model_activations[<span class="st">'stimulus_id'</span>].values) <span class="op">==</span> <span class="bu">set</span>(<span class="va">self</span>._target_assembly[<span class="st">'stimulus_id'</span>].values)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> <span class="va">self</span>.apply_metric(model_activations, <span class="va">self</span>._target_assembly).copy()</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>        normalized_score,raw_score <span class="op">=</span> <span class="va">self</span>.ceiling_normalize(score)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> raw_score, normalized_score</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>benchmark_pool[<span class="st">'Blank2014RandomEmbed'</span>] <span class="op">=</span> BlankRandomEmbed(<span class="st">'Blank2014fROI-encoding'</span>)</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>model<span class="op">=</span><span class="st">'gpt2-xl'</span></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>layers<span class="op">=</span><span class="va">None</span></span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>subsample<span class="op">=</span><span class="va">None</span></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>benchmark<span class="op">=</span><span class="st">'Blank2014RandomEmbed'</span></span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>(raw, normalized )<span class="op">=</span> score_function(model<span class="op">=</span><span class="st">'gpt2-xl'</span>,</span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>            layers<span class="op">=</span>layers,</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>            subsample<span class="op">=</span>subsample,</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>            benchmark<span class="op">=</span>benchmark)</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> normalized.values[:,<span class="dv">0</span>]])</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a><span class="op">----&gt;</span>[<span class="fl">1.6771</span>, <span class="fl">1.7417</span>, <span class="fl">1.5778</span>, <span class="fl">1.7718</span>, <span class="fl">1.6413</span>, <span class="fl">1.6526</span>, <span class="fl">1.6702</span>, <span class="fl">1.6343</span>,</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a> <span class="fl">1.6861</span>, <span class="fl">1.6316</span>, <span class="fl">1.6659</span>, <span class="fl">1.621</span>, <span class="fl">1.7315</span>, <span class="fl">1.7783</span>, <span class="fl">1.7054</span>, <span class="fl">1.6932</span>, <span class="fl">1.5978</span>,</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>  <span class="fl">1.7578</span>, <span class="fl">1.6671</span>, <span class="fl">1.7184</span>, <span class="fl">1.6518</span>, <span class="fl">1.6154</span>, <span class="fl">1.6411</span>, <span class="fl">1.7644</span>, <span class="fl">1.5474</span>, <span class="fl">1.5287</span>, </span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>  ]<span class="fl">1.6716</span>, <span class="fl">1.7877</span>, <span class="fl">1.7792</span>, <span class="fl">1.707</span>, <span class="fl">1.7296</span>, <span class="fl">1.6836</span>, <span class="fl">1.7145</span>, <span class="fl">1.7302</span>, <span class="fl">1.7645</span>, </span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>  <span class="fl">1.7717</span>, <span class="fl">1.568</span>, <span class="fl">1.5788</span>, <span class="fl">1.7494</span>, <span class="fl">1.7074</span>, <span class="fl">1.7392</span>, <span class="fl">1.7495</span>, <span class="fl">1.6844</span>,</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a><span class="fl">1.7248</span>, <span class="fl">1.6814</span>, <span class="fl">1.6247</span>, <span class="fl">1.6582</span>, <span class="fl">1.6814</span>, <span class="fl">1.7328</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="embeddingtrick.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Brain-scores for the random embeddings</figcaption><p></p>
</figure>
</div>
<p>The brain scores here are huge! They completely destroy those of gpt2! So what is going on? If my random embedding trick can do this well within the authors framework, then how can it be said to be adequately testing for anything at all?</p>
</section>
<section id="so-what-allowed-for-the-good-results" class="level3">
<h3 class="anchored" data-anchor-id="so-what-allowed-for-the-good-results">So what allowed for the ‘good’ results?</h3>
<p>My random embedding scheme relied upon the fact that each embedding in a story was ‘similar’ to its parent and child. I did this by injecting only a small amount of noise at each time step. If we think about it, the same thing should apply to gpt2’s activations. The learned representation of the sentence “The quick brown fox jumps over the lazy dog” should be ‘similar’ to “The quick brown fox jumps over the lazy dog and then goes in search of…”. After all, not an awful lot has really changed.</p>
<p>I tried to check this by calculating the 1-step cosine similarity values for the activations gpt2 produced - e.g the similarity between the embedding for “The quick brown fox jumps over the lazy dog” and that for “The quick brown fox jumps over the lazy dog and then goes in search of…” . This didn’t work as expected. The lower (worst performing) layers of gpt2 had the best interstep similarity, which is contrary to what I would have expected from the results. So who knows - maybe cosine similarity just isn’t a useful measure for LLM embeddings.</p>
<p>What I was able to do though, was show that gpt2’s layer activations would exhibit similar behaviour when tested against random autocorrelated signals that I generated off the bat:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> neural_nlp.benchmarks <span class="im">import</span> benchmark_pool</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> pearsonr</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">#Thanks to https://stackoverflow.com/a/33904277 for the sample_signal code</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_signal(n_samples, corr, mu<span class="op">=</span><span class="dv">0</span>, sigma<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="dv">0</span> <span class="op">&lt;</span> corr <span class="op">&lt;</span> <span class="dv">1</span>, <span class="st">"Auto-correlation must be between 0 and 1"</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Find out the offset `c` and the std of the white noise `sigma_e`</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># that produce a signal with the desired mean and variance.</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># See https://en.wikipedia.org/wiki/Autoregressive_model</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># under section "Example: An AR(1) process".</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    c <span class="op">=</span> mu <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> corr)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    sigma_e <span class="op">=</span> np.sqrt((sigma <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> corr <span class="op">**</span> <span class="dv">2</span>))</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample the auto-regressive process.</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    signal <span class="op">=</span> [c <span class="op">+</span> np.random.normal(<span class="dv">0</span>, sigma_e)]</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_samples):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        signal.append(c <span class="op">+</span> corr <span class="op">*</span> signal[<span class="op">-</span><span class="dv">1</span>] <span class="op">+</span> np.random.normal(<span class="dv">0</span>, sigma_e))</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(signal)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_corr_lag_1(signal):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.corrcoef(signal[:<span class="op">-</span><span class="dv">1</span>], signal[<span class="dv">1</span>:])[<span class="dv">0</span>][<span class="dv">1</span>]</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load(f):</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(f, <span class="st">'rb'</span>) <span class="im">as</span> handle:</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pickle.load(handle)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>layer_scores<span class="op">=</span>[]</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co">#create a random autocorrelated signal in the place of each neuroid</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>signal<span class="op">=</span>[ sample_signal(<span class="bu">len</span>(data),<span class="fl">0.5</span>) <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>)]</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">50</span>):</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">#I saved the layer activations to disk.</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>    activations <span class="op">=</span> load(<span class="st">'activations/</span><span class="sc">{}</span><span class="st">.bin'</span>.<span class="bu">format</span>(<span class="bu">str</span>(i))).values</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    corrs<span class="op">=</span>[]</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">50</span>):</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="co">#80/20 train/test plit</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        train_idx <span class="op">=</span> np.random.random(data.shape[<span class="dv">0</span>])<span class="op">&lt;</span><span class="fl">0.8</span> </span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        test_idx <span class="op">=</span> <span class="op">~</span>train_idx</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        train_X <span class="op">=</span> activations[train_idx]</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        train_Y <span class="op">=</span> signal[j].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)[train_idx]</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        test_X <span class="op">=</span> activations[test_idx]</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>        test_Y <span class="op">=</span> signal[j].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)[test_idx]</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        <span class="co">#fit model between gpt2 and the random signal</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> LinearRegression().fit(train_X, train_Y)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        <span class="co">#predict and evaluate</span></span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> model.predict(test_X)</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        cor, p <span class="op">=</span> pearsonr(pred.flatten(), test_Y.flatten())</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>        corrs.append(cor)</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>    layer_scores.append(np.mean(corrs))</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([np.<span class="bu">round</span>(f,<span class="dv">4</span>) <span class="cf">for</span> f <span class="kw">in</span> <span class="bu">list</span>(layer_scores)]) </span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span> [<span class="fl">0.0032</span>, <span class="fl">0.0014</span>, <span class="fl">0.0198</span>, <span class="fl">0.0264</span>, <span class="fl">0.034</span>, <span class="fl">0.0213</span>, <span class="fl">0.0246</span>, <span class="fl">0.0213</span>, <span class="fl">0.0306</span>,</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.03</span>, <span class="fl">0.018</span>, <span class="fl">0.0069</span>, <span class="fl">0.0136</span>, <span class="fl">0.0294</span>, <span class="fl">0.0184</span>, <span class="fl">0.0232</span>, <span class="fl">0.025</span>, <span class="fl">0.0365</span>, <span class="fl">0.0338</span>, </span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0464</span>, <span class="fl">0.0588</span>, <span class="fl">0.0526</span>, <span class="fl">0.0419</span>, <span class="fl">0.0442</span>, <span class="fl">0.0619</span>, <span class="fl">0.0636</span>, <span class="fl">0.0553</span>, <span class="fl">0.0373</span>,</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0368</span>, <span class="fl">0.0485</span>, <span class="fl">0.0614</span>, <span class="fl">0.0494</span>, <span class="fl">0.0541</span>, <span class="fl">0.0519</span>, <span class="fl">0.0507</span>, <span class="fl">0.0511</span>, <span class="fl">0.0563</span>, </span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0651</span>, <span class="fl">0.0482</span>, <span class="fl">0.049</span>, <span class="fl">0.0468</span>, <span class="fl">0.0589</span>, <span class="fl">0.0565</span>, <span class="fl">0.0656</span>, <span class="fl">0.0531</span>, <span class="fl">0.0634</span>, </span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0515</span>, <span class="fl">0.0704</span>, <span class="fl">0.0357</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="randomsignal.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Gpt2 correlation with random signal</figcaption><p></p>
</figure>
</div>
<p>The raw correlation scores get better as you go deeper into the model, and then decrease towards the end. This doesn’t happen quite as smoothly as when fitting gpt2 to the FMRI data. But there you go. Not only can a set of random embeddings excel within the paper’s pipeline, but gpt2 can ‘predict’ more or less any autocorrelated signal, when tested in this way.</p>
</section>
<section id="examining-the-traintest-split." class="level3">
<h3 class="anchored" data-anchor-id="examining-the-traintest-split.">Examining the train/test split.</h3>
<p>You might have noticed that I’ve been performing random train/test splits, with no regards as to the ‘series’ nature of the data. And of course, the authors do this <a href="https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184">too</a>. The option that the authors default to is:</p>
<p><code>self._split = StratifiedKFold(n_splits=splits, shuffle=True, random_state=random_state)</code><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>But this is definitely not how you’re meant to handle time series data. The use of <code>shuffle=True</code> means that the voxel values from the end of each story are mixed up with those at the beginning, and the test values are just randomly missing points in the time series.</p>
<p>If we wanted to just use a consecutive K-Fold cross validation, we could fix it like this:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>._split <span class="op">=</span> KFold(n_splits<span class="op">=</span>splits, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Recomputing is quick, as the previously calculated gpt2 activations have been stored.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> raw.mean(axis<span class="op">=</span><span class="dv">1</span>).values]))</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span>[<span class="fl">0.0099</span>, <span class="fl">0.0065</span>, <span class="fl">0.003</span>, <span class="op">-</span><span class="fl">0.0029</span>, <span class="fl">0.0054</span>, <span class="fl">0.0101</span>, <span class="fl">0.0051</span>, <span class="fl">0.0047</span>, <span class="op">-</span><span class="fl">0.0082</span>, <span class="fl">0.0025</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0047</span>, <span class="fl">0.0088</span>, <span class="fl">0.008</span>, <span class="fl">0.0067</span>, <span class="fl">0.0053</span>, <span class="fl">0.0134</span>, <span class="fl">0.0086</span>, <span class="fl">0.0041</span>, <span class="fl">0.0133</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0174</span>, <span class="fl">0.0131</span>, <span class="fl">0.0053</span>, <span class="fl">0.0045</span>, <span class="fl">0.0043</span>, <span class="fl">0.0075</span>, <span class="fl">0.0127</span>, <span class="fl">0.0147</span>, <span class="fl">0.0166</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.0142</span>, <span class="fl">0.0169</span>, <span class="fl">0.0151</span>, <span class="fl">0.0124</span>, <span class="fl">0.0179</span>, <span class="fl">0.0143</span>, <span class="fl">0.0076</span>, <span class="fl">0.0002</span>, <span class="op">-</span><span class="fl">0.0007</span>, </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>   <span class="op">-</span><span class="fl">0.0005</span>, <span class="op">-</span><span class="fl">0.0002</span>, <span class="fl">0.0005</span>, <span class="op">-</span><span class="fl">0.0019</span>, <span class="op">-</span><span class="fl">0.0001</span>, <span class="op">-</span><span class="fl">0.0018</span>, <span class="fl">0.0008</span>, <span class="fl">0.0022</span>, <span class="op">-</span><span class="fl">0.0002</span>,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.0029</span>, <span class="fl">0.0018</span>, <span class="fl">0.0042</span>]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> normalized.values[:,<span class="dv">0</span>]])</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span>[<span class="fl">0.0486</span>, <span class="fl">0.0586</span>, <span class="fl">0.0251</span>, <span class="op">-</span><span class="fl">0.017</span>, <span class="fl">0.075</span>, <span class="fl">0.0555</span>, <span class="fl">0.0691</span>,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0358</span>, <span class="op">-</span><span class="fl">0.041</span>, <span class="fl">0.0167</span>, <span class="fl">0.0174</span>, <span class="fl">0.0582</span>, <span class="fl">0.0196</span>, <span class="fl">0.0136</span>,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.0446</span>, <span class="fl">0.0415</span>, <span class="fl">0.0772</span>, <span class="fl">0.063</span>, <span class="fl">0.0271</span>, <span class="fl">0.1104</span>, <span class="fl">0.0601</span>,</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.013</span>, <span class="fl">0.0308</span>, <span class="op">-</span><span class="fl">0.031</span>, <span class="fl">0.0151</span>, <span class="fl">0.0978</span>, <span class="fl">0.0963</span>, <span class="fl">0.1133</span>,</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.1328</span>, <span class="fl">0.1853</span>, <span class="fl">0.1706</span>, <span class="fl">0.0971</span>, <span class="fl">0.1552</span>, <span class="fl">0.1403</span>, <span class="fl">0.023</span>, </span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.0135</span>, <span class="fl">0.0109</span>, <span class="op">-</span><span class="fl">0.0029</span>, <span class="op">-</span><span class="fl">0.0107</span>, <span class="op">-</span><span class="fl">0.0227</span>, <span class="op">-</span><span class="fl">0.0169</span>,</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>     <span class="fl">0.0412</span>, <span class="fl">0.0203</span>, <span class="fl">0.0424</span>, <span class="op">-</span><span class="fl">0.0042</span>, <span class="op">-</span><span class="fl">0.0253</span>, <span class="fl">0.0212</span>, <span class="fl">0.0083</span>, <span class="fl">0.0184</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Those scores are really different. But I think there is still some possibility of overlap occurring - future values being used as training data for test values earlier in the series. Another option is to do away with cross validation and just use a single split, using the first 80% of a each story as training data, and the last 20% as test. Thus we ensure that no future data is seen by the linear regressions. This is assuming that the stories constitute breaks in the time series. It’s hacky, but we pop this change into the brain_score code <a href="https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L311">here</a> so that we run the cross validation 5 times on the same split.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>train_idx <span class="op">=</span> []</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>test_idx<span class="op">=</span>[]</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>d<span class="op">=</span>defaultdict(<span class="bu">list</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>stimulus_ids <span class="op">=</span> target_assembly[<span class="st">'stimulus_id'</span>].values</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, stimulus_id <span class="kw">in</span> <span class="bu">enumerate</span>(stimulus_ids):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    stimulus_id<span class="op">=</span>stimulus_id.split(<span class="st">'.'</span>)[<span class="dv">0</span>]</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    d[stimulus_id].append(idx)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> stimulus_id <span class="kw">in</span> d:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    idxs<span class="op">=</span>d[stimulus_id]</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    cutoff<span class="op">=</span><span class="bu">int</span>(<span class="bu">len</span>(idxs)<span class="op">*</span><span class="fl">0.8</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    train_idx.extend(<span class="bu">list</span>(idxs[:cutoff]))</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    test_idx.extend(<span class="bu">list</span>(idxs[cutoff:]))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> split_iterator, (train_indices, test_indices), done <span class="op">\</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>                <span class="kw">in</span> tqdm(enumerate_done(splits), total<span class="op">=</span><span class="bu">len</span>(splits), desc<span class="op">=</span><span class="st">'cross-validation'</span>):</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    train_indices,test_indices <span class="op">=</span> train_idx, test_idx</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> raw.mean(axis<span class="op">=</span><span class="dv">1</span>).values]))</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span>[<span class="fl">0.0022</span>, <span class="fl">0.0179</span>, <span class="fl">0.008</span>, <span class="fl">0.0018</span>, <span class="fl">0.0106</span>, <span class="fl">0.019</span>, <span class="fl">0.0127</span>, <span class="fl">0.0126</span>, <span class="fl">0.0002</span>,</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0037</span>, <span class="fl">0.0047</span>, <span class="fl">0.0111</span>, <span class="fl">0.0089</span>, <span class="fl">0.0147</span>, <span class="fl">0.0069</span>, <span class="fl">0.0161</span>, <span class="fl">0.0131</span>, <span class="fl">0.0064</span>, </span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0164</span>, <span class="fl">0.0191</span>, <span class="fl">0.015</span>, <span class="fl">0.0075</span>, <span class="fl">0.0083</span>, <span class="fl">0.0086</span>, <span class="fl">0.0142</span>, <span class="fl">0.0172</span>, <span class="fl">0.0207</span>, </span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0222</span>, <span class="fl">0.0207</span>, <span class="fl">0.0226</span>, <span class="fl">0.0204</span>, <span class="fl">0.0202</span>, <span class="fl">0.0269</span>, <span class="fl">0.023</span>, <span class="fl">0.0183</span>, <span class="fl">0.0088</span>, </span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0094</span>, <span class="fl">0.0092</span>, <span class="fl">0.0062</span>, <span class="fl">0.0065</span>, <span class="fl">0.0043</span>, <span class="fl">0.0041</span>, <span class="op">-</span><span class="fl">0.0002</span>, <span class="fl">0.0023</span>, <span class="fl">0.0063</span>, </span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0045</span>, <span class="fl">0.005</span>, <span class="fl">0.0034</span>, <span class="fl">0.0083</span>]</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> normalized.values[:,<span class="dv">0</span>]])</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span>[<span class="fl">0.0268</span>, <span class="op">-</span><span class="fl">0.0866</span>, <span class="fl">0.1223</span>, <span class="op">-</span><span class="fl">0.0465</span>, <span class="op">-</span><span class="fl">0.1973</span>, <span class="fl">0.0199</span>, </span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="op">-</span><span class="fl">0.0719</span>, <span class="fl">0.0215</span>, <span class="op">-</span><span class="fl">0.0109</span>, <span class="op">-</span><span class="fl">0.1299</span>, <span class="fl">0.083</span>, <span class="op">-</span><span class="fl">0.0626</span>, <span class="op">-</span><span class="fl">0.0418</span>, </span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a><span class="op">-</span><span class="fl">0.0597</span>, <span class="fl">0.0077</span>, <span class="fl">0.0426</span>, <span class="op">-</span><span class="fl">0.0827</span>, <span class="fl">0.0612</span>, <span class="fl">0.0772</span>, <span class="fl">0.1293</span>, </span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a><span class="fl">0.0654</span>, <span class="op">-</span><span class="fl">0.0816</span>, <span class="fl">0.0846</span>, <span class="op">-</span><span class="fl">0.017</span>, <span class="fl">0.1064</span>, <span class="fl">0.0623</span>, <span class="fl">0.0701</span>, </span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="fl">0.046</span>, <span class="fl">0.1553</span>, <span class="fl">0.1693</span>, <span class="fl">0.0432</span>, <span class="fl">0.1265</span>, <span class="fl">0.008</span>, <span class="fl">0.0444</span>, <span class="fl">0.0082</span>, </span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a><span class="op">-</span><span class="fl">0.0185</span>, <span class="op">-</span><span class="fl">0.0137</span>, <span class="op">-</span><span class="fl">0.0178</span>, <span class="fl">0.0317</span>, <span class="fl">0.0407</span>, <span class="op">-</span><span class="fl">0.0013</span>, <span class="fl">0.0121</span>, </span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a><span class="op">-</span><span class="fl">0.0307</span>, <span class="fl">0.0045</span>, <span class="fl">0.0989</span>, <span class="op">-</span><span class="fl">0.1037</span>, <span class="op">-</span><span class="fl">0.0928</span>, <span class="op">-</span><span class="fl">0.153</span>, <span class="op">-</span><span class="fl">0.0165</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>That’s a big difference. The brain scores have pretty much collapsed, and there are a load of negative scores now. Alternatively, we could consider the whole dataset as one consecutive time series, and train on the first 80% of values and test only on the final 20%. E.g:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> np.arange(target_assembly.shape[<span class="dv">0</span>])</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>cut_off <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(idx)<span class="op">*</span><span class="fl">0.8</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>train_idx <span class="op">=</span> idx[:cut_off]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>test_idx <span class="op">=</span> idx[cut_off:]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">list</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> raw.mean(axis<span class="op">=</span><span class="dv">1</span>).values]))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span>[<span class="fl">0.0014</span>, <span class="fl">0.0081</span>, <span class="fl">0.0092</span>, <span class="op">-</span><span class="fl">0.001</span>, <span class="fl">0.0131</span>, <span class="fl">0.0026</span>, <span class="op">-</span><span class="fl">0.0079</span>, <span class="fl">0.0155</span>, <span class="op">-</span><span class="fl">0.0059</span>, </span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="fl">0.0062</span>, <span class="fl">0.0066</span>, <span class="fl">0.0328</span>, <span class="fl">0.0234</span>, <span class="fl">0.0069</span>, <span class="fl">0.0055</span>, <span class="fl">0.0235</span>, <span class="fl">0.0118</span>, <span class="fl">0.0146</span>, <span class="fl">0.0236</span>, </span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="fl">0.0262</span>, <span class="fl">0.0165</span>, <span class="fl">0.0062</span>, <span class="fl">0.0096</span>, <span class="fl">0.0086</span>, <span class="fl">0.0072</span>, <span class="fl">0.0194</span>, <span class="fl">0.0251</span>, <span class="fl">0.0239</span>, <span class="fl">0.0125</span>, </span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="fl">0.0108</span>, <span class="fl">0.0156</span>, <span class="fl">0.0142</span>, <span class="fl">0.0334</span>, <span class="fl">0.0216</span>, <span class="fl">0.0056</span>, <span class="op">-</span><span class="fl">0.0068</span>, <span class="op">-</span><span class="fl">0.0012</span>, <span class="fl">0.001</span>, <span class="fl">0.0057</span>, </span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="fl">0.0014</span>, <span class="fl">0.0007</span>, <span class="op">-</span><span class="fl">0.0181</span>, <span class="op">-</span><span class="fl">0.0114</span>, <span class="op">-</span><span class="fl">0.0016</span>, <span class="op">-</span><span class="fl">0.0023</span>, <span class="op">-</span><span class="fl">0.0104</span>, <span class="op">-</span><span class="fl">0.009</span>, <span class="op">-</span><span class="fl">0.0104</span>, <span class="op">-</span><span class="fl">0.002</span>]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>([np.<span class="bu">round</span>(x,<span class="dv">4</span>) <span class="cf">for</span> x <span class="kw">in</span> normalized.values[:,<span class="dv">0</span>]])</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="op">---&gt;</span>[<span class="op">-</span><span class="fl">0.0213</span>, <span class="fl">0.0667</span>, <span class="op">-</span><span class="fl">0.0945</span>, <span class="fl">0.0035</span>, <span class="fl">0.0844</span>, <span class="op">-</span><span class="fl">0.0822</span>, <span class="op">-</span><span class="fl">0.0548</span>, <span class="fl">0.18</span>, </span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="fl">0.0561</span>, <span class="fl">0.0135</span>, <span class="fl">0.0509</span>, <span class="fl">0.2229</span>, <span class="fl">0.0795</span>, <span class="fl">0.0935</span>, <span class="fl">0.0614</span>, <span class="fl">0.1577</span>, <span class="fl">0.0676</span>,</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.1072</span>, <span class="fl">0.1658</span>, <span class="fl">0.1828</span>, <span class="fl">0.0529</span>, <span class="fl">0.0734</span>, <span class="fl">0.0523</span>, <span class="fl">0.0425</span>, <span class="fl">0.0354</span>, <span class="fl">0.126</span>, </span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.1824</span>, <span class="fl">0.0505</span>, <span class="fl">0.0899</span>, <span class="fl">0.0414</span>, <span class="fl">0.0949</span>, <span class="fl">0.124</span>, <span class="fl">0.2261</span>, <span class="fl">0.2791</span>, <span class="fl">0.1468</span>, <span class="fl">0.0557</span>, </span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.0382</span>, <span class="fl">0.0193</span>, <span class="fl">0.0594</span>, <span class="fl">0.0374</span>, <span class="fl">0.0412</span>, <span class="op">-</span><span class="fl">0.02</span>, <span class="op">-</span><span class="fl">0.0065</span>, <span class="op">-</span><span class="fl">0.0035</span>, <span class="fl">0.0318</span>, <span class="op">-</span><span class="fl">0.0479</span>, <span class="op">-</span><span class="fl">0.0127</span>, <span class="fl">0.0144</span>, <span class="fl">0.0567</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="finalcomparison.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Brain score comparisons across split strategies</figcaption><p></p>
</figure>
</div>
<p>As the gpt2 activations have been saved, it all recomputes pretty quickly, and we get the above results. The mean normalized brain_score across layers in the first instance was <code>0.21432</code>. For the KFold fix, it went down to <code>0.044738</code>. Splitting the stories gave us <code>0.008828</code>, and just naively splitting the whole dataset 80/20 gives us <code>0.06575102040816326</code>. And look at the raw correlation values! There are still one or two layers showing some layer/brain correlation, but the values are tiny. But, a correlation is still a correlation. Or is it?</p>
</section>
<section id="what-makes-a-significant-brain-score" class="level3">
<h3 class="anchored" data-anchor-id="what-makes-a-significant-brain-score">What makes a significant brain-score?</h3>
<p>I wanted to look a bit further, so I unpacked the actual p-values for the individual correlations that the best scoring gpt2 layer produced.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>activations<span class="op">=</span>load(<span class="st">'activations/</span><span class="sc">{}</span><span class="st">.bin'</span>.<span class="bu">format</span>(<span class="bu">str</span>(best_layer<span class="op">+</span><span class="dv">1</span>))).values</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> np.arange(data.shape[<span class="dv">0</span>])</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>cut_off <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(idx)<span class="op">*</span><span class="fl">0.8</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>train_idx<span class="op">=</span>idx[:cut_off]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>test_idx<span class="op">=</span>idx[cut_off:]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>train_X <span class="op">=</span> activations[train_idx]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>train_Y <span class="op">=</span> data.values[train_idx]</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>test_X <span class="op">=</span> activations[test_idx]</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>test_Y <span class="op">=</span> data.values[test_idx]</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression().fit(train_X, train_Y)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> model.predict(test_X)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>ps<span class="op">=</span>[]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(data.shape[<span class="dv">1</span>]):</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    cor, p <span class="op">=</span> pearsonr(pred[:,j].flatten(), test_Y[:,j].flatten())</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    ps.append(p)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ps)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.mean(ps))</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="op">----&gt;</span>[<span class="fl">0.1261</span>, <span class="fl">0.4546</span>, <span class="fl">0.7403</span>, <span class="fl">0.4028</span>, <span class="fl">0.1986</span>, <span class="fl">0.8898</span>, <span class="fl">0.2407</span>,</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.5314</span>, <span class="fl">0.4364</span>, <span class="fl">0.9006</span>, <span class="fl">0.8181</span>, <span class="fl">0.4686</span>, <span class="fl">0.0237</span>, <span class="fl">0.3228</span>, <span class="fl">0.7242</span>,</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>  <span class="fl">0.3558</span>, <span class="fl">0.0549</span>, <span class="fl">0.1904</span>, <span class="fl">0.9157</span>, <span class="fl">0.3441</span>, <span class="fl">0.1256</span>, <span class="fl">0.0802</span>, <span class="fl">0.5643</span>,</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>   <span class="fl">0.4628</span>, <span class="fl">0.1521</span>, <span class="fl">0.9165</span>, <span class="fl">0.2372</span>, <span class="fl">0.6676</span>, <span class="fl">0.9878</span>, <span class="fl">0.9566</span>, <span class="fl">0.6633</span>,</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    <span class="fl">0.1939</span>, <span class="fl">0.2056</span>, <span class="fl">0.6876</span>, <span class="fl">0.7831</span>, <span class="fl">0.2193</span>, <span class="fl">0.9157</span>, <span class="fl">0.0956</span>, <span class="fl">0.8299</span>,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>     <span class="fl">0.867</span>, <span class="fl">0.4778</span>, <span class="fl">0.8489</span>, <span class="fl">0.6799</span>, <span class="fl">0.3197</span>, <span class="fl">0.7199</span>, <span class="fl">0.7147</span>, <span class="fl">0.6371</span>, </span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>     <span class="fl">0.8556</span>, <span class="fl">0.374</span>, <span class="fl">0.103</span>, <span class="fl">0.8697</span>, <span class="fl">0.2803</span>, <span class="fl">0.4908</span>, <span class="fl">0.0162</span>, <span class="fl">0.7662</span>,</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>      <span class="fl">0.3224</span>, <span class="fl">0.6696</span>, <span class="fl">0.0839</span>, <span class="fl">0.0171</span>, <span class="fl">0.9656</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>So they’re really high. Really, really high. If the significance threshold is a conservative 0.05, only 3 voxels actually test out below that. Most of them look like garbage.</p>
<p>But what do we do with these p-values? How can we judge the significance of the overall brain-score (average correlation)?</p>
<p>A straight up approach might suggest that we’re testing multiple different hypotheses (e.g gpt2 correlates with voxel 1, with voxel 2… etc). If we follow the <a href="https://en.wikipedia.org/wiki/Bonferroni_correction">Bonferroni correction</a> method, then we would end up setting our significance threshold 60 times lower, and reject everything. But that can’t be how people generally treat brain voxels right?</p>
<p>As per wikipedia, multiple comparisons assumes that we are testing quite different hypotheses e.g a new teaching method improves spelling, arithmetic, reading comprehension. But these voxels, they’re kind of like similar hypotheses right? So perhaps the p-value for the final brain score should end up lower overall?</p>
<p>I couldn’t decide. I thought maybe we could empirically estimate p-values for the brain-scores by running the test thousands of times but with randomly generated gpt2 activations. Then we can see an approximate likelihood of doing as well as gpt2 with completely uncorrelated data. It’s a bit rough and ready, but I guess it will do in the absence of something better.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>brain_scores <span class="op">=</span> []</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the average correlation between random activations and the brain data</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.arange(data.shape[<span class="dv">0</span>])</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    cut_off <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(idx)<span class="op">*</span><span class="fl">0.8</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    train_idx<span class="op">=</span>idx[:cut_off]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    test_idx<span class="op">=</span>idx[cut_off:]</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    activations <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, activations.shape)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    train_X <span class="op">=</span> activations[train_idx]</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    train_Y <span class="op">=</span> data.values[train_idx]</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    test_X <span class="op">=</span> activations[test_idx]</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    test_Y <span class="op">=</span> data.values[test_idx]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> LinearRegression().fit(train_X, train_Y)</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> model.predict(test_X)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    corrs<span class="op">=</span>[]</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    ps<span class="op">=</span>[]</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(data.shape[<span class="dv">1</span>]):</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        cor, p <span class="op">=</span> pearsonr(pred[:,j].flatten(), test_Y[:,j].flatten())</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        corrs.append(cor)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>    brain_scores.append(np.mean(corrs))</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co">#sort the average correlations and plot as p-value/average-correlation</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>sorted_brain_scores<span class="op">=</span><span class="bu">sorted</span>(brain_scores)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>sorted_brain_scores<span class="op">=</span>np.flip(sorted_brain_scores)[:<span class="dv">200</span>]</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>x<span class="op">=</span>np.arange(<span class="bu">len</span>(sorted_brain_scores))<span class="op">*</span><span class="fl">0.001</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>plt.plot(x, sorted_brain_scores)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p-value'</span>)</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'correlation'</span>)</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<figure class="figure">
<img src="pvalues2.png" title="Title" class="img-fluid figure-img">
</figure>
<p></p><figcaption class="figure-caption">Average-Correlation/p-value plot</figcaption><p></p>
</figure>
</div>
<p>So if that’s right, an average_correlation at or above <code>0.0195</code> is ‘significant’, at <code>p&lt;0.05</code>. That puts several layers of gpt2 well within significance territory.</p>
<p>Still, doesn’t testing 50 different gpt2 activation layers sound an awful lot like testing 50 different hypothesis? If so, then applying Bonferroni correction we’d be adjusting the significance threshold down to <code>p&lt;=0.001</code>, or an average correlation of <code>0.0412</code>. That way we’d once again end up rejecting everything.</p>
<p>I can’t decide. I guess gpt2 might correlate with your brain, but only very, very slightly.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>Altogether, this paper hasn’t convinced me that gpt2 knows anything about my brain. There are some caveats though. As mentioned<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, I couldn’t get the package to install cleanly, so I would be a little wary of version mismatches. The choice of train/test split was a pretty obvious mistake, but I am unsure as to whether what’s left afterwards is significant or not. Also, I have only tested one dataset, whereas the authors provided four in their code. I think the loader for Federenko-2016 wouldn’t work, and Pereira-2018 with its 150,000 FMRI values was going to take days to compute for. The same dubious <code>StratifiedKFold</code> is used for all four datasets though, so I would question the effect size of the results given in the paper. That said, the results could remain significant once this has been corrected for.</p>
<p>Also, this doesn’t say anything about the many other papers out there that are doing similar things. For instance, Caucheteux et al (code unavailable) state that they use a <code>GroupKFold</code>, which sounds a lot better. They also present much lower brain_scores, though no explicit p-values for them, and no mention of how they addressed the multiple comparisons thing.</p>
<p>If I get some time, I might try to compute for the other datasets here. But for now, it seems inconclusive. Overall, I hope that gpt2-brain researchers can agree upon a reasonable way to do a train/test split. They could also do better at explaining to lay people what are acceptable underlying correlations and p-values for the normalized brain-score values that they present as evidence.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-caucheteux2022deep" class="csl-entry" role="doc-biblioentry">
Caucheteux, Charlotte, Alexandre Gramfort, and Jean-Rémi King. 2022. <span>“Deep Language Algorithms Predict Semantic Comprehension from Brain Activity.”</span> <em>Scientific Reports</em> 12 (1): 1–10.
</div>
<div id="ref-schrimpf2021neural" class="csl-entry" role="doc-biblioentry">
Schrimpf, Martin, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. 2021. <span>“The Neural Architecture of Language: Integrative Modeling Converges on Predictive Processing.”</span> <em>Proceedings of the National Academy of Sciences</em> 118 (45): e2105646118.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The funny thing is that anyone who downloaded and ran gpt2 back in 2019 will remember that it didn’t really seem to understand language at all.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In the dataset examined here, <a href="https://journals.physiology.org/doi/full/10.1152/jn.00884.2013">Blank et al 2014</a>, there are only 60 values per time point. I assume this means that the values have been aggregated accross regions and participants, and restricted to only a set of voxels that are relevant for language processing. I’m not sure though. Anecdotally, I have heard that this FMRI thing can be something of a dark art.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I had some problems installing this. In the end, what seemed to work was installing nltk, pytorch and tensorflow separately with pip/conda, grabbing the latest <a href="https://github.com/brain-score/brain-score/tree/master/brainscore">brainscore</a> and <a href="https://github.com/brain-score/brainio_base">brainio_base</a> packages directly from github, and only then running the neural_nlp setup.py script.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><a href="https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184">https://github.com/brain-score/brain-score/blob/master/brainscore/metrics/transformations.py#L184</a> This line in the brainscore code predates the release of the paper by at least 2 years, so it seems like they did in fact use it to compute their paper results.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>See [1]<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>